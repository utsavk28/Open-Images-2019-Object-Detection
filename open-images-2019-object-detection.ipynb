{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightgbm catboost -qq\n!pip install gdown -qq\n!gdown --id 1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT\n!git clone https://github.com/taipingeric/yolo-v4-tf.keras","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":10.150343,"end_time":"2022-02-16T08:55:53.278859","exception":false,"start_time":"2022-02-16T08:55:43.128516","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-18T04:58:44.389009Z","iopub.execute_input":"2022-02-18T04:58:44.389371Z","iopub.status.idle":"2022-02-18T04:59:25.729641Z","shell.execute_reply.started":"2022-02-18T04:58:44.389282Z","shell.execute_reply":"2022-02-18T04:59:25.728693Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/taipingeric/yolo-v4-tf.keras","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:59:25.731421Z","iopub.execute_input":"2022-02-18T04:59:25.731651Z","iopub.status.idle":"2022-02-18T04:59:26.480783Z","shell.execute_reply.started":"2022-02-18T04:59:25.731622Z","shell.execute_reply":"2022-02-18T04:59:26.479948Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Machine Learning Imports\n\nimport os\nimport xgboost as xgb\nimport lightgbm as lgm\nimport catboost as cb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler,Binarizer,MaxAbsScaler,Normalizer\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,OrdinalEncoder\nfrom sklearn.preprocessing import PolynomialFeatures,PowerTransformer,QuantileTransformer\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile\nfrom sklearn.feature_selection import chi2,f_classif,f_regression,mutual_info_classif,mutual_info_regression\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,HuberRegressor\nfrom sklearn.naive_bayes import BernoulliNB,CategoricalNB,ComplementNB,GaussianNB,MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.svm import LinearSVC,LinearSVR,NuSVC,NuSVR,SVR,SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor,ExtraTreeClassifier,ExtraTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,VotingRegressor\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,VotingClassifier\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import accuracy_score,f1_score,mean_squared_error,mean_absolute_error,r2_score","metadata":{"papermill":{"duration":2.691008,"end_time":"2022-02-16T08:55:55.977531","exception":false,"start_time":"2022-02-16T08:55:53.286523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-18T04:59:26.482899Z","iopub.execute_input":"2022-02-18T04:59:26.483250Z","iopub.status.idle":"2022-02-18T04:59:29.141164Z","shell.execute_reply.started":"2022-02-18T04:59:26.483207Z","shell.execute_reply":"2022-02-18T04:59:29.140388Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Utils Imports\n\nimport cv2\n\nvocab = {\"/m/011k07\": \"Tortoise\", \"/m/011q46kg\": \"Container\", \"/m/012074\": \"Magpie\", \"/m/0120dh\": \"Sea turtle\", \"/m/01226z\": \"Football\", \"/m/012n7d\": \"Ambulance\", \"/m/012w5l\": \"Ladder\", \"/m/012xff\": \"Toothbrush\", \"/m/012ysf\": \"Syringe\", \"/m/0130jx\": \"Sink\", \"/m/0138tl\": \"Toy\", \"/m/013y1f\": \"Organ\", \"/m/01432t\": \"Cassette deck\", \"/m/014j1m\": \"Apple\", \"/m/014sv8\": \"Human eye\", \"/m/014trl\": \"Cosmetics\", \"/m/014y4n\": \"Paddle\", \"/m/0152hh\": \"Snowman\", \"/m/01599\": \"Beer\", \"/m/01_5g\": \"Chopsticks\", \"/m/015h_t\": \"Human beard\", \"/m/015p6\": \"Bird\", \"/m/015qbp\": \"Parking meter\", \"/m/015qff\": \"Traffic light\", \"/m/015wgc\": \"Croissant\", \"/m/015x4r\": \"Cucumber\", \"/m/015x5n\": \"Radish\", \"/m/0162_1\": \"Towel\", \"/m/0167gd\": \"Doll\", \"/m/016m2d\": \"Skull\", \"/m/0174k2\": \"Washing machine\", \"/m/0174n1\": \"Glove\", \"/m/0175cv\": \"Tick\", \"/m/0176mf\": \"Belt\", \"/m/017ftj\": \"Sunglasses\", \"/m/018j2\": \"Banjo\", \"/m/018p4k\": \"Cart\", \"/m/018xm\": \"Ball\", \"/m/01940j\": \"Backpack\", \"/m/0199g\": \"Bicycle\", \"/m/019dx1\": \"Home appliance\", \"/m/019h78\": \"Centipede\", \"/m/019jd\": \"Boat\", \"/m/019w40\": \"Surfboard\", \"/m/01b638\": \"Boot\", \"/m/01b7fy\": \"Headphones\", \"/m/01b9xk\": \"Hot dog\", \"/m/01bfm9\": \"Shorts\", \"/m/01_bhs\": \"Fast food\", \"/m/01bjv\": \"Bus\", \"/m/01bl7v\": \"Boy\", \"/m/01bms0\": \"Screwdriver\", \"/m/01bqk0\": \"Bicycle wheel\", \"/m/01btn\": \"Barge\", \"/m/01c648\": \"Laptop\", \"/m/01cmb2\": \"Miniskirt\", \"/m/01d380\": \"Drill\", \"/m/01d40f\": \"Dress\", \"/m/01dws\": \"Bear\", \"/m/01dwsz\": \"Waffle\", \"/m/01dwwc\": \"Pancake\", \"/m/01dxs\": \"Brown bear\", \"/m/01dy8n\": \"Woodpecker\", \"/m/01f8m5\": \"Blue jay\", \"/m/01f91_\": \"Pretzel\", \"/m/01fb_0\": \"Bagel\", \"/m/01fdzj\": \"Tower\", \"/m/01fh4r\": \"Teapot\", \"/m/01g317\": \"Person\", \"/m/01g3x7\": \"Bow and arrow\", \"/m/01gkx_\": \"Swimwear\", \"/m/01gllr\": \"Beehive\", \"/m/01gmv2\": \"Brassiere\", \"/m/01h3n\": \"Bee\", \"/m/01h44\": \"Bat\", \"/m/01h8tj\": \"Starfish\", \"/m/01hrv5\": \"Popcorn\", \"/m/01j3zr\": \"Burrito\", \"/m/01j4z9\": \"Chainsaw\", \"/m/01j51\": \"Balloon\", \"/m/01j5ks\": \"Wrench\", \"/m/01j61q\": \"Tent\", \"/m/01jfm_\": \"Vehicle registration plate\", \"/m/01jfsr\": \"Lantern\", \"/m/01k6s3\": \"Toaster\", \"/m/01kb5b\": \"Flashlight\", \"/m/01knjb\": \"Billboard\", \"/m/01krhy\": \"Tiara\", \"/m/01lcw4\": \"Limousine\", \"/m/01llwg\": \"Necklace\", \"/m/01lrl\": \"Carnivore\", \"/m/01lsmm\": \"Scissors\", \"/m/01lynh\": \"Stairs\", \"/m/01m2v\": \"Computer keyboard\", \"/m/01m4t\": \"Printer\", \"/m/01mqdt\": \"Traffic sign\", \"/m/01mzpv\": \"Chair\", \"/m/01n4qj\": \"Shirt\", \"/m/01n5jq\": \"Poster\", \"/m/01nkt\": \"Cheese\", \"/m/01nq26\": \"Sock\", \"/m/01pns0\": \"Fire hydrant\", \"/m/01prls\": \"Land vehicle\", \"/m/01r546\": \"Earrings\", \"/m/01rkbr\": \"Tie\", \"/m/01rzcn\": \"Watercraft\", \"/m/01s105\": \"Cabinetry\", \"/m/01s55n\": \"Suitcase\", \"/m/01tcjp\": \"Muffin\", \"/m/01vbnl\": \"Bidet\", \"/m/01ww8y\": \"Snack\", \"/m/01x3jk\": \"Snowmobile\", \"/m/01x3z\": \"Clock\", \"/m/01xgg_\": \"Medical equipment\", \"/m/01xq0k1\": \"Cattle\", \"/m/01xqw\": \"Cello\", \"/m/01xs3r\": \"Jet ski\", \"/m/01x_v\": \"Camel\", \"/m/01xygc\": \"Coat\", \"/m/01xyhv\": \"Suit\", \"/m/01y9k5\": \"Desk\", \"/m/01yrx\": \"Cat\", \"/m/01yx86\": \"Bronze sculpture\", \"/m/01z1kdw\": \"Juice\", \"/m/02068x\": \"Gondola\", \"/m/020jm\": \"Beetle\", \"/m/020kz\": \"Cannon\", \"/m/020lf\": \"Computer mouse\", \"/m/021mn\": \"Cookie\", \"/m/021sj1\": \"Office building\", \"/m/0220r2\": \"Fountain\", \"/m/0242l\": \"Coin\", \"/m/024d2\": \"Calculator\", \"/m/024g6\": \"Cocktail\", \"/m/02522\": \"Computer monitor\", \"/m/025dyy\": \"Box\", \"/m/025fsf\": \"Stapler\", \"/m/025nd\": \"Christmas tree\", \"/m/025rp__\": \"Cowboy hat\", \"/m/0268lbt\": \"Hiking equipment\", \"/m/026qbn5\": \"Studio couch\", \"/m/026t6\": \"Drum\", \"/m/0270h\": \"Dessert\", \"/m/0271qf7\": \"Wine rack\", \"/m/0271t\": \"Drink\", \"/m/027pcv\": \"Zucchini\", \"/m/027rl48\": \"Ladle\", \"/m/0283dt1\": \"Human mouth\", \"/m/0284d\": \"Dairy\", \"/m/029b3\": \"Dice\", \"/m/029bxz\": \"Oven\", \"/m/029tx\": \"Dinosaur\", \"/m/02bm9n\": \"Ratchet\", \"/m/02crq1\": \"Couch\", \"/m/02ctlc\": \"Cricket ball\", \"/m/02cvgx\": \"Winter melon\", \"/m/02d1br\": \"Spatula\", \"/m/02d9qx\": \"Whiteboard\", \"/m/02ddwp\": \"Pencil sharpener\", \"/m/02dgv\": \"Door\", \"/m/02dl1y\": \"Hat\", \"/m/02f9f_\": \"Shower\", \"/m/02fh7f\": \"Eraser\", \"/m/02fq_6\": \"Fedora\", \"/m/02g30s\": \"Guacamole\", \"/m/02gzp\": \"Dagger\", \"/m/02h19r\": \"Scarf\", \"/m/02hj4\": \"Dolphin\", \"/m/02jfl0\": \"Sombrero\", \"/m/02jnhm\": \"Tin can\", \"/m/02jvh9\": \"Mug\", \"/m/02jz0l\": \"Tap\", \"/m/02l8p9\": \"Harbor seal\", \"/m/02lbcq\": \"Stretcher\", \"/m/02mqfb\": \"Can opener\", \"/m/02_n6y\": \"Goggles\", \"/m/02p0tk3\": \"Human body\", \"/m/02p3w7d\": \"Roller skates\", \"/m/02p5f1q\": \"Cup\", \"/m/02pdsw\": \"Cutting board\", \"/m/02pjr4\": \"Blender\", \"/m/02pkr5\": \"Plumbing fixture\", \"/m/02pv19\": \"Stop sign\", \"/m/02rdsp\": \"Office supplies\", \"/m/02rgn06\": \"Volleyball\", \"/m/02s195\": \"Vase\", \"/m/02tsc9\": \"Slow cooker\", \"/m/02vkqh8\": \"Wardrobe\", \"/m/02vqfm\": \"Coffee\", \"/m/02vwcm\": \"Whisk\", \"/m/02w3r3\": \"Paper towel\", \"/m/02w3_ws\": \"Personal care\", \"/m/02wbm\": \"Food\", \"/m/02wbtzl\": \"Sun hat\", \"/m/02wg_p\": \"Tree house\", \"/m/02wmf\": \"Flying disc\", \"/m/02wv6h6\": \"Skirt\", \"/m/02wv84t\": \"Gas stove\", \"/m/02x8cch\": \"Salt and pepper shakers\", \"/m/02x984l\": \"Mechanical fan\", \"/m/02xb7qb\": \"Face powder\", \"/m/02xqq\": \"Fax\", \"/m/02xwb\": \"Fruit\", \"/m/02y6n\": \"French fries\", \"/m/02z51p\": \"Nightstand\", \"/m/02zn6n\": \"Barrel\", \"/m/02zt3\": \"Kite\", \"/m/02zvsm\": \"Tart\", \"/m/030610\": \"Treadmill\", \"/m/0306r\": \"Fox\", \"/m/03120\": \"Flag\", \"/m/0319l\": \"Horn\", \"/m/031b6r\": \"Window blind\", \"/m/031n1\": \"Human foot\", \"/m/0323sq\": \"Golf cart\", \"/m/032b3c\": \"Jacket\", \"/m/033cnk\": \"Egg\", \"/m/033rq4\": \"Street light\", \"/m/0342h\": \"Guitar\", \"/m/034c16\": \"Pillow\", \"/m/035r7c\": \"Human leg\", \"/m/035vxb\": \"Isopod\", \"/m/0388q\": \"Grape\", \"/m/039xj_\": \"Human ear\", \"/m/03bbps\": \"Power plugs and sockets\", \"/m/03bj1\": \"Panda\", \"/m/03bk1\": \"Giraffe\", \"/m/03bt1vf\": \"Woman\", \"/m/03c7gz\": \"Door handle\", \"/m/03d443\": \"Rhinoceros\", \"/m/03dnzn\": \"Bathtub\", \"/m/03fj2\": \"Goldfish\", \"/m/03fp41\": \"Houseplant\", \"/m/03fwl\": \"Goat\", \"/m/03g8mr\": \"Baseball bat\", \"/m/03grzl\": \"Baseball glove\", \"/m/03hj559\": \"Mixing bowl\", \"/m/03hl4l9\": \"Marine invertebrates\", \"/m/03hlz0c\": \"Kitchen utensil\", \"/m/03jbxj\": \"Light switch\", \"/m/03jm5\": \"House\", \"/m/03k3r\": \"Horse\", \"/m/03kt2w\": \"Stationary bicycle\", \"/m/03l9g\": \"Hammer\", \"/m/03ldnb\": \"Ceiling fan\", \"/m/03m3pdh\": \"Sofa bed\", \"/m/03m3vtv\": \"Adhesive tape\", \"/m/03m5k\": \"Harp\", \"/m/03nfch\": \"Sandal\", \"/m/03p3bw\": \"Bicycle helmet\", \"/m/03q5c7\": \"Saucer\", \"/m/03q5t\": \"Harpsichord\", \"/m/03q69\": \"Human hair\", \"/m/03qhv5\": \"Heater\", \"/m/03qjg\": \"Harmonica\", \"/m/03qrc\": \"Hamster\", \"/m/03rszm\": \"Curtain\", \"/m/03ssj5\": \"Bed\", \"/m/03s_tn\": \"Kettle\", \"/m/03tw93\": \"Fireplace\", \"/m/03txqz\": \"Scale\", \"/m/03v5tg\": \"Drinking straw\", \"/m/03vt0\": \"Insect\", \"/m/03wvsk\": \"Hair dryer\", \"/m/03_wxk\": \"Kitchenware\", \"/m/03wym\": \"Indoor rower\", \"/m/03xxp\": \"Invertebrate\", \"/m/03y6mg\": \"Food processor\", \"/m/03__z0\": \"Bookcase\", \"/m/040b_t\": \"Refrigerator\", \"/m/04169hn\": \"Wood-burning stove\", \"/m/0420v5\": \"Punching bag\", \"/m/043nyj\": \"Common fig\", \"/m/0440zs\": \"Cocktail shaker\", \"/m/0449p\": \"Jaguar\", \"/m/044r5d\": \"Golf ball\", \"/m/0463sg\": \"Fashion accessory\", \"/m/046dlr\": \"Alarm clock\", \"/m/047j0r\": \"Filing cabinet\", \"/m/047v4b\": \"Artichoke\", \"/m/04bcr3\": \"Table\", \"/m/04brg2\": \"Tableware\", \"/m/04c0y\": \"Kangaroo\", \"/m/04cp_\": \"Koala\", \"/m/04ctx\": \"Knife\", \"/m/04dr76w\": \"Bottle\", \"/m/04f5ws\": \"Bottle opener\", \"/m/04g2r\": \"Lynx\", \"/m/04gth\": \"Lavender\", \"/m/04h7h\": \"Lighthouse\", \"/m/04h8sr\": \"Dumbbell\", \"/m/04hgtk\": \"Human head\", \"/m/04kkgm\": \"Bowl\", \"/m/04lvq_\": \"Humidifier\", \"/m/04m6gz\": \"Porch\", \"/m/04m9y\": \"Lizard\", \"/m/04p0qw\": \"Billiard table\", \"/m/04rky\": \"Mammal\", \"/m/04rmv\": \"Mouse\", \"/m/04_sv\": \"Motorcycle\", \"/m/04szw\": \"Musical instrument\", \"/m/04tn4x\": \"Swim cap\", \"/m/04v6l4\": \"Frying pan\", \"/m/04vv5k\": \"Snowplow\", \"/m/04y4h8h\": \"Bathroom cabinet\", \"/m/04ylt\": \"Missile\", \"/m/04yqq2\": \"Bust\", \"/m/04yx4\": \"Man\", \"/m/04z4wx\": \"Waffle iron\", \"/m/04zpv\": \"Milk\", \"/m/04zwwv\": \"Ring binder\", \"/m/050gv4\": \"Plate\", \"/m/050k8\": \"Mobile phone\", \"/m/052lwg6\": \"Baked goods\", \"/m/052sf\": \"Mushroom\", \"/m/05441v\": \"Crutch\", \"/m/054fyh\": \"Pitcher\", \"/m/054_l\": \"Mirror\", \"/m/054xkw\": \"Lifejacket\", \"/m/05_5p_0\": \"Table tennis racket\", \"/m/05676x\": \"Pencil case\", \"/m/057cc\": \"Musical keyboard\", \"/m/057p5t\": \"Scoreboard\", \"/m/0584n8\": \"Briefcase\", \"/m/058qzx\": \"Kitchen knife\", \"/m/05bm6\": \"Nail\", \"/m/05ctyq\": \"Tennis ball\", \"/m/05gqfk\": \"Plastic bag\", \"/m/05kms\": \"Oboe\", \"/m/05kyg_\": \"Chest of drawers\", \"/m/05n4y\": \"Ostrich\", \"/m/05r5c\": \"Piano\", \"/m/05r655\": \"Girl\", \"/m/05s2s\": \"Plant\", \"/m/05vtc\": \"Potato\", \"/m/05w9t9\": \"Hair spray\", \"/m/05y5lj\": \"Sports equipment\", \"/m/05z55\": \"Pasta\", \"/m/05z6w\": \"Penguin\", \"/m/05zsy\": \"Pumpkin\", \"/m/061_f\": \"Pear\", \"/m/061hd_\": \"Infant bed\", \"/m/0633h\": \"Polar bear\", \"/m/063rgb\": \"Mixer\", \"/m/0642b4\": \"Cupboard\", \"/m/065h6l\": \"Jacuzzi\", \"/m/0663v\": \"Pizza\", \"/m/06_72j\": \"Digital clock\", \"/m/068zj\": \"Pig\", \"/m/06bt6\": \"Reptile\", \"/m/06c54\": \"Rifle\", \"/m/06c7f7\": \"Lipstick\", \"/m/06_fw\": \"Skateboard\", \"/m/06j2d\": \"Raven\", \"/m/06k2mb\": \"High heels\", \"/m/06l9r\": \"Red panda\", \"/m/06m11\": \"Rose\", \"/m/06mf6\": \"Rabbit\", \"/m/06msq\": \"Sculpture\", \"/m/06ncr\": \"Saxophone\", \"/m/06nrc\": \"Shotgun\", \"/m/06nwz\": \"Seafood\", \"/m/06pcq\": \"Submarine sandwich\", \"/m/06__v\": \"Snowboard\", \"/m/06y5r\": \"Sword\", \"/m/06z37_\": \"Picture frame\", \"/m/07030\": \"Sushi\", \"/m/0703r8\": \"Loveseat\", \"/m/071p9\": \"Ski\", \"/m/071qp\": \"Squirrel\", \"/m/073bxn\": \"Tripod\", \"/m/073g6\": \"Stethoscope\", \"/m/074d1\": \"Submarine\", \"/m/0755b\": \"Scorpion\", \"/m/076bq\": \"Segway\", \"/m/076lb9\": \"Training bench\", \"/m/078jl\": \"Snake\", \"/m/078n6m\": \"Coffee table\", \"/m/079cl\": \"Skyscraper\", \"/m/07bgp\": \"Sheep\", \"/m/07c52\": \"Television\", \"/m/07c6l\": \"Trombone\", \"/m/07clx\": \"Tea\", \"/m/07cmd\": \"Tank\", \"/m/07crc\": \"Taco\", \"/m/07cx4\": \"Telephone\", \"/m/07dd4\": \"Torch\", \"/m/07dm6\": \"Tiger\", \"/m/07fbm7\": \"Strawberry\", \"/m/07gql\": \"Trumpet\", \"/m/07j7r\": \"Tree\", \"/m/07j87\": \"Tomato\", \"/m/07jdr\": \"Train\", \"/m/07k1x\": \"Tool\", \"/m/07kng9\": \"Picnic basket\", \"/m/07mcwg\": \"Cooking spray\", \"/m/07mhn\": \"Trousers\", \"/m/07pj7bq\": \"Bowling equipment\", \"/m/07qxg_\": \"Football helmet\", \"/m/07r04\": \"Truck\", \n         \"/m/07v9_z\": \"Measuring cup\", \"/m/07xyvk\": \"Coffeemaker\", \"/m/07y_7\": \"Violin\", \"/m/07yv9\": \"Vehicle\", \"/m/080hkjn\": \"Handbag\", \"/m/080n7g\": \"Paper cutter\", \"/m/081qc\": \"Wine\", \"/m/083kb\": \"Weapon\", \"/m/083wq\": \"Wheel\", \"/m/084hf\": \"Worm\", \"/m/084rd\": \"Wok\", \"/m/084zz\": \"Whale\", \"/m/0898b\": \"Zebra\", \"/m/08dz3q\": \"Auto part\", \"/m/08hvt4\": \"Jug\", \"/m/08ks85\": \"Pizza cutter\", \"/m/08p92x\": \"Cream\", \"/m/08pbxl\": \"Monkey\", \"/m/096mb\": \"Lion\", \"/m/09728\": \"Bread\", \"/m/099ssp\": \"Platter\", \"/m/09b5t\": \"Chicken\", \"/m/09csl\": \"Eagle\", \"/m/09ct_\": \"Helicopter\", \"/m/09d5_\": \"Owl\", \"/m/09ddx\": \"Duck\", \"/m/09dzg\": \"Turtle\", \"/m/09f20\": \"Hippopotamus\", \"/m/09f_2\": \"Crocodile\", \"/m/09g1w\": \"Toilet\", \"/m/09gtd\": \"Toilet paper\", \"/m/09gys\": \"Squid\", \"/m/09j2d\": \"Clothing\", \"/m/09j5n\": \"Footwear\", \"/m/09k_b\": \"Lemon\", \"/m/09kmb\": \"Spider\", \"/m/09kx5\": \"Deer\", \"/m/09ld4\": \"Frog\", \"/m/09qck\": \"Banana\", \"/m/09rvcxw\": \"Rocket\", \"/m/09tvcd\": \"Wine glass\", \"/m/0b3fp9\": \"Countertop\", \"/m/0bh9flk\": \"Tablet computer\", \"/m/0bjyj5\": \"Waste container\", \"/m/0b_rs\": \"Swimming pool\", \"/m/0bt9lr\": \"Dog\", \"/m/0bt_c3\": \"Book\", \"/m/0bwd_0j\": \"Elephant\", \"/m/0by6g\": \"Shark\", \"/m/0c06p\": \"Candle\", \"/m/0c29q\": \"Leopard\", \"/m/0c2jj\": \"Axe\", \"/m/0c3m8g\": \"Hand dryer\", \"/m/0c3mkw\": \"Soap dispenser\", \"/m/0c568\": \"Porcupine\", \"/m/0c9ph5\": \"Flower\", \"/m/0ccs93\": \"Canary\", \"/m/0cd4d\": \"Cheetah\", \"/m/0cdl1\": \"Palm tree\", \"/m/0cdn1\": \"Hamburger\", \"/m/0cffdh\": \"Maple\", \"/m/0cgh4\": \"Building\", \"/m/0ch_cf\": \"Fish\", \"/m/0cjq5\": \"Lobster\", \"/m/0cjs7\": \"Asparagus\", \"/m/0c_jw\": \"Furniture\", \"/m/0cl4p\": \"Hedgehog\", \"/m/0cmf2\": \"Aeroplane\", \"/m/0cmx8\": \"Spoon\", \"/m/0cn6p\": \"Otter\", \"/m/0cnyhnx\": \"Bull\", \"/m/0_cp5\": \"Oyster\", \"/m/0cqn2\": \"Horizontal bar\", \"/m/0crjs\": \"Convenience store\", \"/m/0ct4f\": \"Bomb\", \"/m/0cvnqh\": \"Bench\", \"/m/0cxn2\": \"Ice cream\", \"/m/0cydv\": \"Caterpillar\", \"/m/0cyf8\": \"Butterfly\", \"/m/0cyfs\": \"Parachute\", \"/m/0cyhj_\": \"Orange\", \"/m/0czz2\": \"Antelope\", \"/m/0d20w4\": \"Beaker\", \"/m/0d_2m\": \"Moths and butterflies\", \"/m/0d4v4\": \"Window\", \"/m/0d4w1\": \"Closet\", \"/m/0d5gx\": \"Castle\", \"/m/0d8zb\": \"Jellyfish\", \"/m/0dbvp\": \"Goose\", \"/m/0dbzx\": \"Mule\", \"/m/0dftk\": \"Swan\", \"/m/0dj6p\": \"Peach\", \"/m/0djtd\": \"Coconut\", \"/m/0dkzw\": \"Seat belt\", \"/m/0dq75\": \"Raccoon\", \"/m/0_dqb\": \"Chisel\", \"/m/0dt3t\": \"Fork\", \"/m/0dtln\": \"Lamp\", \"/m/0dv5r\": \"Camera\", \"/m/0dv77\": \"Squash\", \"/m/0dv9c\": \"Racket\", \"/m/0dzct\": \"Human face\", \"/m/0dzf4\": \"Human arm\", \"/m/0f4s2w\": \"Vegetable\", \"/m/0f571\": \"Diaper\", \"/m/0f6nr\": \"Unicycle\", \"/m/0f6wt\": \"Falcon\", \"/m/0f8s22\": \"Chime\", \"/m/0f9_l\": \"Snail\", \"/m/0fbdv\": \"Shellfish\", \"/m/0fbw6\": \"Cabbage\", \"/m/0fj52s\": \"Carrot\", \"/m/0fldg\": \"Mango\", \"/m/0fly7\": \"Jeans\", \"/m/0fm3zh\": \"Flowerpot\", \"/m/0fp6w\": \"Pineapple\", \"/m/0fqfqc\": \"Drawer\", \"/m/0fqt361\": \"Stool\", \"/m/0frqm\": \"Envelope\", \"/m/0fszt\": \"Cake\", \"/m/0ft9s\": \"Dragonfly\", \"/m/0ftb8\": \"Sunflower\", \"/m/0fx9l\": \"Microwave oven\", \"/m/0fz0h\": \"Honeycomb\", \"/m/0gd2v\": \"Marine mammal\", \"/m/0gd36\": \"Sea lion\", \"/m/0gj37\": \"Ladybug\", \"/m/0gjbg72\": \"Shelf\", \"/m/0gjkl\": \"Watch\", \"/m/0gm28\": \"Candy\", \"/m/0grw1\": \"Salad\", \"/m/0gv1x\": \"Parrot\", \"/m/0gxl3\": \"Handgun\", \"/m/0h23m\": \"Sparrow\", \"/m/0h2r6\": \"Van\", \"/m/0h8jyh6\": \"Grinder\", \"/m/0h8kx63\": \"Spice rack\", \"/m/0h8l4fh\": \"Light bulb\", \"/m/0h8lkj8\": \"Corded phone\", \"/m/0h8mhzd\": \"Sports uniform\", \"/m/0h8my_4\": \"Tennis racket\", \"/m/0h8mzrc\": \"Wall clock\", \"/m/0h8n27j\": \"Serving tray\", \"/m/0h8n5zk\": \"Dining table\", \"/m/0h8n6f9\": \"Dog bed\", \"/m/0h8n6ft\": \"Cake stand\", \"/m/0h8nm9j\": \"Cat furniture\", \"/m/0h8nr_l\": \"Bathroom accessory\", \"/m/0h8nsvg\": \"Facial tissue holder\", \"/m/0h8ntjv\": \"Pressure cooker\", \"/m/0h99cwc\": \"Kitchen appliance\", \"/m/0h9mv\": \"Tire\", \"/m/0hdln\": \"Ruler\", \"/m/0hf58v5\": \"Luggage and bags\", \"/m/0hg7b\": \"Microphone\", \"/m/0hkxq\": \"Broccoli\", \"/m/0hnnb\": \"Umbrella\", \"/m/0hnyx\": \"Pastry\", \"/m/0hqkz\": \"Grapefruit\", \"/m/0j496\": \"Band-aid\", \"/m/0jbk\": \"Animal\", \"/m/0jg57\": \"Bell pepper\", \"/m/0jly1\": \"Turkey\", \"/m/0jqgx\": \"Lily\", \"/m/0jwn_\": \"Pomegranate\", \"/m/0jy4k\": \"Doughnut\", \"/m/0jyfg\": \"Glasses\", \"/m/0k0pj\": \"Human nose\", \"/m/0k1tl\": \"Pen\", \"/m/0_k2\": \"Ant\", \"/m/0k4j\": \"Car\", \"/m/0k5j\": \"Aircraft\", \"/m/0k65p\": \"Human hand\", \"/m/0km7z\": \"Skunk\", \"/m/0kmg4\": \"Teddy bear\", \"/m/0kpqd\": \"Watermelon\", \"/m/0kpt_\": \"Cantaloupe\", \"/m/0ky7b\": \"Dishwasher\", \"/m/0l14j_\": \"Flute\", \"/m/0l3ms\": \"Balance beam\", \"/m/0l515\": \"Sandwich\", \"/m/0ll1f78\": \"Shrimp\", \"/m/0llzx\": \"Sewing machine\", \"/m/0lt4_\": \"Binoculars\", \"/m/0m53l\": \"Rays and skates\", \"/m/0mcx2\": \"Ipod\", \"/m/0mkg\": \"Accordion\", \"/m/0mw_6\": \"Willow\", \"/m/0n28_\": \"Crab\", \"/m/0nl46\": \"Crown\", \"/m/0nybt\": \"Seahorse\", \"/m/0p833\": \"Perfume\", \"/m/0pcr\": \"Alpaca\", \"/m/0pg52\": \"Taxi\", \"/m/0ph39\": \"Canoe\", \"/m/0qjjc\": \"Remote control\", \"/m/0qmmr\": \"Wheelchair\", \"/m/0wdt60w\": \"Rugby ball\", \"/m/0xfy\": \"Armadillo\", \"/m/0xzly\": \"Maracas\", \"/m/0zvk5\": \"Helmet\"}\nrev = {}\nfor k,v in vocab.items():\n    rev[v.lower()] = k ","metadata":{"papermill":{"duration":0.361533,"end_time":"2022-02-16T08:56:01.834083","exception":false,"start_time":"2022-02-16T08:56:01.47255","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-18T04:59:29.143581Z","iopub.execute_input":"2022-02-18T04:59:29.143806Z","iopub.status.idle":"2022-02-18T04:59:29.522059Z","shell.execute_reply.started":"2022-02-18T04:59:29.143779Z","shell.execute_reply":"2022-02-18T04:59:29.521409Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Yolo Code","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nimport json\nfrom tqdm import tqdm\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras import layers, initializers, models\n\n\nimport operator\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import Sequence\n\nimport math\nimport tensorflow.keras.backend as K\n\n\ndef load_weights(model, weights_file_path):\n    conv_layer_size = 110\n    conv_output_idxs = [93, 101, 109]\n    with open(weights_file_path, 'rb') as file:\n        major, minor, revision, seen, _ = np.fromfile(file, dtype=np.int32, count=5)\n\n        bn_idx = 0\n        for conv_idx in range(conv_layer_size):\n            conv_layer_name = f'conv2d_{conv_idx}' if conv_idx > 0 else 'conv2d'\n            bn_layer_name = f'batch_normalization_{bn_idx}' if bn_idx > 0 else 'batch_normalization'\n\n            conv_layer = model.get_layer(conv_layer_name)\n            filters = conv_layer.filters\n            kernel_size = conv_layer.kernel_size[0]\n            input_dims = conv_layer.input_shape[-1]\n\n            if conv_idx not in conv_output_idxs:\n                # darknet bn layer weights: [beta, gamma, mean, variance]\n                bn_weights = np.fromfile(file, dtype=np.float32, count=4 * filters)\n                # tf bn layer weights: [gamma, beta, mean, variance]\n                bn_weights = bn_weights.reshape((4, filters))[[1, 0, 2, 3]]\n                bn_layer = model.get_layer(bn_layer_name)\n                bn_idx += 1\n            else:\n                conv_bias = np.fromfile(file, dtype=np.float32, count=filters)\n\n            # darknet shape: (out_dim, input_dims, height, width)\n            # tf shape: (height, width, input_dims, out_dim)\n            conv_shape = (filters, input_dims, kernel_size, kernel_size)\n            conv_weights = np.fromfile(file, dtype=np.float32, count=np.product(conv_shape))\n            conv_weights = conv_weights.reshape(conv_shape).transpose([2, 3, 1, 0])\n\n            if conv_idx not in conv_output_idxs:\n                conv_layer.set_weights([conv_weights])\n                bn_layer.set_weights(bn_weights)\n            else:\n                conv_layer.set_weights([conv_weights, conv_bias])\n\n        if len(file.read()) == 0:\n            print('all weights read')\n        else:\n            print(f'failed to read  all weights, # of unread weights: {len(file.read())}')\n\n\ndef get_detection_data(img, model_outputs, class_names):\n    \"\"\"\n    :param img: target raw image\n    :param model_outputs: outputs from inference_model\n    :param class_names: list of object class names\n    :return:\n    \"\"\"\n\n    num_bboxes = model_outputs[-1][0]\n    boxes, scores, classes = [output[0][:num_bboxes] for output in model_outputs[:-1]]\n\n    h, w = img.shape[:2]\n    df = pd.DataFrame(boxes, columns=['x1', 'y1', 'x2', 'y2'])\n    df[['x1', 'x2']] = (df[['x1', 'x2']] * w).astype('int64')\n    df[['y1', 'y2']] = (df[['y1', 'y2']] * h).astype('int64')\n    df['class_name'] = np.array(class_names)[classes.astype('int64')]\n    df['score'] = scores\n    df['w'] = df['x2'] - df['x1']\n    df['h'] = df['y2'] - df['y1']\n\n#     print(f'# of bboxes: {num_bboxes}')\n    return df\n\ndef read_annotation_lines(annotation_path, test_size=None, random_seed=5566):\n    with open(annotation_path) as f:\n        lines = f.readlines()\n    if test_size:\n        return train_test_split(lines, test_size=test_size, random_state=random_seed)\n    else:\n        return lines\n\ndef draw_bbox(img, detections, cmap, random_color=True, figsize=(10, 10), show_img=True, show_text=True):\n    \"\"\"\n    Draw bounding boxes on the img.\n    :param img: BGR img.\n    :param detections: pandas DataFrame containing detections\n    :param random_color: assign random color for each objects\n    :param cmap: object colormap\n    :param plot_img: if plot img with bboxes\n    :return: None\n    \"\"\"\n    img = np.array(img)\n    scale = max(img.shape[0:2]) / 416\n    line_width = int(2 * scale)\n\n    for _, row in detections.iterrows():\n        x1, y1, x2, y2, cls, score, w, h = row.values\n        color = list(np.random.random(size=3) * 255) if random_color else cmap[cls]\n        cv2.rectangle(img, (x1, y1), (x2, y2), color, line_width)\n        if show_text:\n            text = f'{cls} {score:.2f}'\n            font = cv2.FONT_HERSHEY_DUPLEX\n            font_scale = max(0.3 * scale, 0.3)\n            thickness = max(int(1 * scale), 1)\n            (text_width, text_height) = cv2.getTextSize(text, font, fontScale=font_scale, thickness=thickness)[0]\n            cv2.rectangle(img, (x1 - line_width//2, y1 - text_height), (x1 + text_width, y1), color, cv2.FILLED)\n            cv2.putText(img, text, (x1, y1), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n    if show_img:\n        plt.figure(figsize=figsize)\n        plt.imshow(img)\n        plt.show()\n    return img\n\n\nclass DataGenerator(Sequence):\n    \"\"\"\n    Generates data for Keras\n    ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n    \"\"\"\n    def __init__(self,\n                 annotation_lines,\n                 class_name_path,\n                 folder_path,\n                 max_boxes=100,\n                 shuffle=True):\n        self.annotation_lines = annotation_lines\n        self.class_name_path = class_name_path\n        self.num_classes = len([line.strip() for line in open(class_name_path).readlines()])\n        self.num_gpu = yolo_config['num_gpu']\n        self.batch_size = yolo_config['batch_size'] * self.num_gpu\n        self.target_img_size = yolo_config['img_size']\n        self.anchors = np.array(yolo_config['anchors']).reshape((9, 2))\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(self.annotation_lines))\n        self.folder_path = folder_path\n        self.max_boxes = max_boxes\n        self.on_epoch_end()\n\n    def __len__(self):\n        'number of batches per epoch'\n        return int(np.ceil(len(self.annotation_lines) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n\n        # Generate indexes of the batch\n        idxs = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Find list of IDs\n        lines = [self.annotation_lines[i] for i in idxs]\n\n        # Generate data\n        X, y_tensor, y_bbox = self.__data_generation(lines)\n\n        return [X, *y_tensor, y_bbox], np.zeros(len(lines))\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, annotation_lines):\n        \"\"\"\n        Generates data containing batch_size samples\n        :param annotation_lines:\n        :return:\n        \"\"\"\n\n        X = np.empty((len(annotation_lines), *self.target_img_size), dtype=np.float32)\n        y_bbox = np.empty((len(annotation_lines), self.max_boxes, 5), dtype=np.float32)  # x1y1x2y2\n\n        for i, line in enumerate(annotation_lines):\n            img_data, box_data = self.get_data(line)\n            X[i] = img_data\n            y_bbox[i] = box_data\n\n        y_tensor, y_true_boxes_xywh = preprocess_true_boxes(y_bbox, self.target_img_size[:2], self.anchors, self.num_classes)\n\n        return X, y_tensor, y_true_boxes_xywh\n\n    def get_data(self, annotation_line):\n        line = annotation_line.split()\n        img_path = line[0]\n        img = cv2.imread(os.path.join(self.folder_path, img_path))[:, :, ::-1]\n        ih, iw = img.shape[:2]\n        h, w, c = self.target_img_size\n        boxes = np.array([np.array(list(map(float, box.split(',')))) for box in line[1:]], dtype=np.float32) # x1y1x2y2\n        scale_w, scale_h = w / iw, h / ih\n        img = cv2.resize(img, (w, h))\n        image_data = np.array(img) / 255.\n\n        # correct boxes coordinates\n        box_data = np.zeros((self.max_boxes, 5))\n        if len(boxes) > 0:\n            np.random.shuffle(boxes)\n            boxes = boxes[:self.max_boxes]\n            boxes[:, [0, 2]] = boxes[:, [0, 2]] * scale_w  # + dx\n            boxes[:, [1, 3]] = boxes[:, [1, 3]] * scale_h  # + dy\n            box_data[:len(boxes)] = boxes\n\n        return image_data, box_data\n\n\ndef preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n    '''Preprocess true boxes to training input format\n    Parameters\n    ----------\n    true_boxes: array, shape=(bs, max boxes per img, 5)\n        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n    input_shape: array-like, hw, multiples of 32\n    anchors: array, shape=(N, 2), (9, wh)\n    num_classes: int\n    Returns\n    -------\n    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n    '''\n\n    num_stages = 3  # default setting for yolo, tiny yolo will be 2\n    anchor_mask = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    bbox_per_grid = 3\n    true_boxes = np.array(true_boxes, dtype='float32')\n    true_boxes_abs = np.array(true_boxes, dtype='float32')\n    input_shape = np.array(input_shape, dtype='int32')\n    true_boxes_xy = (true_boxes_abs[..., 0:2] + true_boxes_abs[..., 2:4]) // 2  # (100, 2)\n    true_boxes_wh = true_boxes_abs[..., 2:4] - true_boxes_abs[..., 0:2]  # (100, 2)\n\n    # Normalize x,y,w, h, relative to img size -> (0~1)\n    true_boxes[..., 0:2] = true_boxes_xy/input_shape[::-1]  # xy\n    true_boxes[..., 2:4] = true_boxes_wh/input_shape[::-1]  # wh\n\n    bs = true_boxes.shape[0]\n    grid_sizes = [input_shape//{0:8, 1:16, 2:32}[stage] for stage in range(num_stages)]\n    y_true = [np.zeros((bs,\n                        grid_sizes[s][0],\n                        grid_sizes[s][1],\n                        bbox_per_grid,\n                        5+num_classes), dtype='float32')\n              for s in range(num_stages)]\n    # [(?, 52, 52, 3, 5+num_classes) (?, 26, 26, 3, 5+num_classes)  (?, 13, 13, 3, 5+num_classes) ]\n    y_true_boxes_xywh = np.concatenate((true_boxes_xy, true_boxes_wh), axis=-1)\n    # Expand dim to apply broadcasting.\n    anchors = np.expand_dims(anchors, 0)  # (1, 9 , 2)\n    anchor_maxes = anchors / 2.  # (1, 9 , 2)\n    anchor_mins = -anchor_maxes  # (1, 9 , 2)\n    valid_mask = true_boxes_wh[..., 0] > 0  # (1, 100)\n\n    for batch_idx in range(bs):\n        # Discard zero rows.\n        wh = true_boxes_wh[batch_idx, valid_mask[batch_idx]]  # (# of bbox, 2)\n        num_boxes = len(wh)\n        if num_boxes == 0: continue\n        wh = np.expand_dims(wh, -2)  # (# of bbox, 1, 2)\n        box_maxes = wh / 2.  # (# of bbox, 1, 2)\n        box_mins = -box_maxes  # (# of bbox, 1, 2)\n\n        # Compute IoU between each anchors and true boxes for responsibility assignment\n        intersect_mins = np.maximum(box_mins, anchor_mins)  # (# of bbox, 9, 2)\n        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n        intersect_area = np.prod(intersect_wh, axis=-1)  # (9,)\n        box_area = wh[..., 0] * wh[..., 1]  # (# of bbox, 1)\n        anchor_area = anchors[..., 0] * anchors[..., 1]  # (1, 9)\n        iou = intersect_area / (box_area + anchor_area - intersect_area)  # (# of bbox, 9)\n\n        # Find best anchor for each true box\n        best_anchors = np.argmax(iou, axis=-1)  # (# of bbox,)\n        for box_idx in range(num_boxes):\n            best_anchor = best_anchors[box_idx]\n            for stage in range(num_stages):\n                if best_anchor in anchor_mask[stage]:\n                    x_offset = true_boxes[batch_idx, box_idx, 0]*grid_sizes[stage][1]\n                    y_offset = true_boxes[batch_idx, box_idx, 1]*grid_sizes[stage][0]\n                    # Grid Index\n                    grid_col = np.floor(x_offset).astype('int32')\n                    grid_row = np.floor(y_offset).astype('int32')\n                    anchor_idx = anchor_mask[stage].index(best_anchor)\n                    class_idx = true_boxes[batch_idx, box_idx, 4].astype('int32')\n                    # y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, 0] = x_offset - grid_col  # x\n                    # y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, 1] = y_offset - grid_row  # y\n                    # y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, :4] = true_boxes_abs[batch_idx, box_idx, :4] # abs xywh\n                    y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, :2] = true_boxes_xy[batch_idx, box_idx, :]  # abs xy\n                    y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, 2:4] = true_boxes_wh[batch_idx, box_idx, :]  # abs wh\n                    y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, 4] = 1  # confidence\n\n                    y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, 5+class_idx] = 1  # one-hot encoding\n                    # smooth\n                    # onehot = np.zeros(num_classes, dtype=np.float)\n                    # onehot[class_idx] = 1.0\n                    # uniform_distribution = np.full(num_classes, 1.0 / num_classes)\n                    # delta = 0.01\n                    # smooth_onehot = onehot * (1 - delta) + delta * uniform_distribution\n                    # y_true[stage][batch_idx, grid_row, grid_col, anchor_idx, 5:] = smooth_onehot\n\n    return y_true, y_true_boxes_xywh\n\n\"\"\"\n Calculate the AP given the recall and precision array\n    1st) We compute a version of the measured precision/recall curve with\n         precision monotonically decreasing\n    2nd) We compute the AP as the area under this curve by numerical integration.\n\"\"\"\ndef voc_ap(rec, prec):\n    \"\"\"\n    --- Official matlab code VOC2012---\n    mrec=[0 ; rec ; 1];\n    mpre=[0 ; prec ; 0];\n    for i=numel(mpre)-1:-1:1\n            mpre(i)=max(mpre(i),mpre(i+1));\n    end\n    i=find(mrec(2:end)~=mrec(1:end-1))+1;\n    ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n    \"\"\"\n    rec.insert(0, 0.0) # insert 0.0 at begining of list\n    rec.append(1.0) # insert 1.0 at end of list\n    mrec = rec[:]\n    prec.insert(0, 0.0) # insert 0.0 at begining of list\n    prec.append(0.0) # insert 0.0 at end of list\n    mpre = prec[:]\n    \"\"\"\n     This part makes the precision monotonically decreasing\n        (goes from the end to the beginning)\n        matlab: for i=numel(mpre)-1:-1:1\n                    mpre(i)=max(mpre(i),mpre(i+1));\n    \"\"\"\n    # matlab indexes start in 1 but python in 0, so I have to do:\n    #     range(start=(len(mpre) - 2), end=0, step=-1)\n    # also the python function range excludes the end, resulting in:\n    #     range(start=(len(mpre) - 2), end=-1, step=-1)\n    for i in range(len(mpre)-2, -1, -1):\n        mpre[i] = max(mpre[i], mpre[i+1])\n    \"\"\"\n     This part creates a list of indexes where the recall changes\n        matlab: i=find(mrec(2:end)~=mrec(1:end-1))+1;\n    \"\"\"\n    i_list = []\n    for i in range(1, len(mrec)):\n        if mrec[i] != mrec[i-1]:\n            i_list.append(i) # if it was matlab would be i + 1\n    \"\"\"\n     The Average Precision (AP) is the area under the curve\n        (numerical integration)\n        matlab: ap=sum((mrec(i)-mrec(i-1)).*mpre(i));\n    \"\"\"\n    ap = 0.0\n    for i in i_list:\n        ap += ((mrec[i]-mrec[i-1])*mpre[i])\n    return ap, mrec, mpre\n\n\"\"\"\n Draw plot using Matplotlib\n\"\"\"\ndef draw_plot_func(dictionary, n_classes, window_title, plot_title, x_label, output_path, to_show, plot_color, true_p_bar):\n    # sort the dictionary by decreasing value, into a list of tuples\n    sorted_dic_by_value = sorted(dictionary.items(), key=operator.itemgetter(1))\n#     print(sorted_dic_by_value)\n    # unpacking the list of tuples into two lists\n    sorted_keys, sorted_values = zip(*sorted_dic_by_value)\n    #\n    if true_p_bar != \"\":\n        \"\"\"\n         Special case to draw in:\n            - green -> TP: True Positives (object detected and matches ground-truth)\n            - red -> FP: False Positives (object detected but does not match ground-truth)\n            - pink -> FN: False Negatives (object not detected but present in the ground-truth)\n        \"\"\"\n        fp_sorted = []\n        tp_sorted = []\n        for key in sorted_keys:\n            fp_sorted.append(dictionary[key] - true_p_bar[key])\n            tp_sorted.append(true_p_bar[key])\n        plt.barh(range(n_classes), fp_sorted, align='center', color='crimson', label='False Positive')\n        plt.barh(range(n_classes), tp_sorted, align='center', color='forestgreen', label='True Positive', left=fp_sorted)\n        # add legend\n        plt.legend(loc='lower right')\n        \"\"\"\n         Write number on side of bar\n        \"\"\"\n        fig = plt.gcf() # gcf - get current figure\n        axes = plt.gca()\n        r = fig.canvas.get_renderer()\n        for i, val in enumerate(sorted_values):\n            fp_val = fp_sorted[i]\n            tp_val = tp_sorted[i]\n            fp_str_val = \" \" + str(fp_val)\n            tp_str_val = fp_str_val + \" \" + str(tp_val)\n            # trick to paint multicolor with offset:\n            # first paint everything and then repaint the first number\n            t = plt.text(val, i, tp_str_val, color='forestgreen', va='center', fontweight='bold')\n            plt.text(val, i, fp_str_val, color='crimson', va='center', fontweight='bold')\n            if i == (len(sorted_values)-1): # largest bar\n                adjust_axes(r, t, fig, axes)\n    else:\n        plt.barh(range(n_classes), sorted_values, color=plot_color)\n        \"\"\"\n         Write number on side of bar\n        \"\"\"\n        fig = plt.gcf() # gcf - get current figure\n        axes = plt.gca()\n        r = fig.canvas.get_renderer()\n        for i, val in enumerate(sorted_values):\n            str_val = \" \" + str(val) # add a space before\n            if val < 1.0:\n                str_val = \" {0:.2f}\".format(val)\n            t = plt.text(val, i, str_val, color=plot_color, va='center', fontweight='bold')\n            # re-set axes to show number inside the figure\n            if i == (len(sorted_values)-1): # largest bar\n                adjust_axes(r, t, fig, axes)\n    # set window title\n    fig.canvas.set_window_title(window_title)\n    # write classes in y axis\n    tick_font_size = 12\n    plt.yticks(range(n_classes), sorted_keys, fontsize=tick_font_size)\n    \"\"\"\n     Re-scale height accordingly\n    \"\"\"\n    init_height = fig.get_figheight()\n    # comput the matrix height in points and inches\n    dpi = fig.dpi\n    height_pt = n_classes * (tick_font_size * 1.4) # 1.4 (some spacing)\n    height_in = height_pt / dpi\n    # compute the required figure height\n    top_margin = 0.15 # in percentage of the figure height\n    bottom_margin = 0.05 # in percentage of the figure height\n    figure_height = height_in / (1 - top_margin - bottom_margin)\n    # set new height\n    if figure_height > init_height:\n        fig.set_figheight(figure_height)\n\n    # set plot title\n    plt.title(plot_title, fontsize=14)\n    # set axis titles\n    # plt.xlabel('classes')\n    plt.xlabel(x_label, fontsize='large')\n    # adjust size of window\n    fig.tight_layout()\n    # save the plot\n    fig.savefig(output_path)\n    # show image\n    # if to_show:\n    plt.show()\n    # close the plot\n    # plt.close()\n\n\"\"\"\n Plot - adjust axes\n\"\"\"\ndef adjust_axes(r, t, fig, axes):\n    # get text width for re-scaling\n    bb = t.get_window_extent(renderer=r)\n    text_width_inches = bb.width / fig.dpi\n    # get axis width in inches\n    current_fig_width = fig.get_figwidth()\n    new_fig_width = current_fig_width + text_width_inches\n    propotion = new_fig_width / current_fig_width\n    # get axis limit\n    x_lim = axes.get_xlim()\n    axes.set_xlim([x_lim[0], x_lim[1]*propotion])\n\n\ndef read_txt_to_list(path):\n    # open txt file lines to a list\n    with open(path) as f:\n        content = f.readlines()\n    # remove whitespace characters like `\\n` at the end of each line\n    content = [x.strip() for x in content]\n    return content\n\ndef xywh_to_x1y1x2y2(boxes):\n    return tf.concat([boxes[..., :2] - boxes[..., 2:] * 0.5, boxes[..., :2] + boxes[..., 2:] * 0.5], axis=-1)\n\n\n# x,y,w,h\ndef bbox_iou(boxes1, boxes2):\n    boxes1_area = boxes1[..., 2] * boxes1[..., 3]  # w * h\n    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n    # (x, y, w, h) -> (x0, y0, x1, y1)\n    boxes1 = xywh_to_x1y1x2y2(boxes1)\n    boxes2 = xywh_to_x1y1x2y2(boxes2)\n\n    # coordinates of intersection\n    top_left = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n    bottom_right = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n    intersection_xy = tf.maximum(bottom_right - top_left, 0.0)\n\n    intersection_area = intersection_xy[..., 0] * intersection_xy[..., 1]\n    union_area = boxes1_area + boxes2_area - intersection_area\n\n    return 1.0 * intersection_area / (union_area + tf.keras.backend.epsilon())\n\n\ndef bbox_giou(boxes1, boxes2):\n    boxes1_area = boxes1[..., 2] * boxes1[..., 3]  # w*h\n    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n\n    # (x, y, w, h) -> (x0, y0, x1, y1)\n    boxes1 = xywh_to_x1y1x2y2(boxes1)\n    boxes2 = xywh_to_x1y1x2y2(boxes2)\n\n    top_left = tf.maximum(boxes1[..., :2], boxes2[..., :2])\n    bottom_right = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\n\n    intersection_xy = tf.maximum(bottom_right - top_left, 0.0)\n    intersection_area = intersection_xy[..., 0] * intersection_xy[..., 1]\n\n    union_area = boxes1_area + boxes2_area - intersection_area\n\n    iou = 1.0 * intersection_area / (union_area + tf.keras.backend.epsilon())\n\n    enclose_top_left = tf.minimum(boxes1[..., :2], boxes2[..., :2])\n    enclose_bottom_right = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\n\n    enclose_xy = enclose_bottom_right - enclose_top_left\n    enclose_area = enclose_xy[..., 0] * enclose_xy[..., 1]\n\n    giou = iou - tf.math.divide_no_nan(enclose_area - union_area, enclose_area)\n\n    return giou\n\n\ndef bbox_ciou(boxes1, boxes2):\n    '''\n    ciou = iou - p2/c2 - av\n    :param boxes1: (8, 13, 13, 3, 4)   pred_xywh\n    :param boxes2: (8, 13, 13, 3, 4)   label_xywh\n    :return:\n    '''\n    boxes1_x0y0x1y1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n                                 boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n    boxes2_x0y0x1y1 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n                                 boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n    boxes1_x0y0x1y1 = tf.concat([tf.minimum(boxes1_x0y0x1y1[..., :2], boxes1_x0y0x1y1[..., 2:]),\n                                 tf.maximum(boxes1_x0y0x1y1[..., :2], boxes1_x0y0x1y1[..., 2:])], axis=-1)\n    boxes2_x0y0x1y1 = tf.concat([tf.minimum(boxes2_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., 2:]),\n                                 tf.maximum(boxes2_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., 2:])], axis=-1)\n\n    # area\n    boxes1_area = (boxes1_x0y0x1y1[..., 2] - boxes1_x0y0x1y1[..., 0]) * (\n                boxes1_x0y0x1y1[..., 3] - boxes1_x0y0x1y1[..., 1])\n    boxes2_area = (boxes2_x0y0x1y1[..., 2] - boxes2_x0y0x1y1[..., 0]) * (\n                boxes2_x0y0x1y1[..., 3] - boxes2_x0y0x1y1[..., 1])\n\n    # top-left and bottom-right coord, shape: (8, 13, 13, 3, 2)\n    left_up = tf.maximum(boxes1_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., :2])\n    right_down = tf.minimum(boxes1_x0y0x1y1[..., 2:], boxes2_x0y0x1y1[..., 2:])\n\n    # intersection area and iou\n    inter_section = tf.maximum(right_down - left_up, 0.0)\n    inter_area = inter_section[..., 0] * inter_section[..., 1]\n    union_area = boxes1_area + boxes2_area - inter_area\n    iou = inter_area / (union_area + 1e-9)\n\n    # top-left and bottom-right coord of the enclosing rectangle, shape: (8, 13, 13, 3, 2)\n    enclose_left_up = tf.minimum(boxes1_x0y0x1y1[..., :2], boxes2_x0y0x1y1[..., :2])\n    enclose_right_down = tf.maximum(boxes1_x0y0x1y1[..., 2:], boxes2_x0y0x1y1[..., 2:])\n\n    # diagnal ** 2\n    enclose_wh = enclose_right_down - enclose_left_up\n    enclose_c2 = K.pow(enclose_wh[..., 0], 2) + K.pow(enclose_wh[..., 1], 2)\n\n    # center distances between two rectangles\n    p2 = K.pow(boxes1[..., 0] - boxes2[..., 0], 2) + K.pow(boxes1[..., 1] - boxes2[..., 1], 2)\n\n    # add av\n    atan1 = tf.atan(boxes1[..., 2] / (boxes1[..., 3] + 1e-9))\n    atan2 = tf.atan(boxes2[..., 2] / (boxes2[..., 3] + 1e-9))\n    v = 4.0 * K.pow(atan1 - atan2, 2) / (math.pi ** 2)\n    a = v / (1 - iou + v)\n\n    ciou = iou - 1.0 * p2 / enclose_c2 - 1.0 * a * v\n    return ciou\n\n\ndef yolo_loss(args, num_classes, iou_loss_thresh, anchors):\n    conv_lbbox = args[2]   # (?, ?, ?, 3*(num_classes+5))\n    conv_mbbox = args[1]   # (?, ?, ?, 3*(num_classes+5))\n    conv_sbbox = args[0]   # (?, ?, ?, 3*(num_classes+5))\n    label_sbbox = args[3]   # (?, ?, ?, 3, num_classes+5)\n    label_mbbox = args[4]   # (?, ?, ?, 3, num_classes+5)\n    label_lbbox = args[5]   # (?, ?, ?, 3, num_classes+5)\n    true_bboxes = args[6]   # (?, 50, 4)\n    pred_sbbox = decode(conv_sbbox, anchors[0], 8, num_classes)\n    pred_mbbox = decode(conv_mbbox, anchors[1], 16, num_classes)\n    pred_lbbox = decode(conv_lbbox, anchors[2], 32, num_classes)\n    sbbox_ciou_loss, sbbox_conf_loss, sbbox_prob_loss = loss_layer(conv_sbbox, pred_sbbox, label_sbbox, true_bboxes, 8, num_classes, iou_loss_thresh)\n    mbbox_ciou_loss, mbbox_conf_loss, mbbox_prob_loss = loss_layer(conv_mbbox, pred_mbbox, label_mbbox, true_bboxes, 16, num_classes, iou_loss_thresh)\n    lbbox_ciou_loss, lbbox_conf_loss, lbbox_prob_loss = loss_layer(conv_lbbox, pred_lbbox, label_lbbox, true_bboxes, 32, num_classes, iou_loss_thresh)\n\n    ciou_loss = (lbbox_ciou_loss + sbbox_ciou_loss + mbbox_ciou_loss) * 3.54\n    conf_loss = (lbbox_conf_loss + sbbox_conf_loss + mbbox_conf_loss) * 64.3\n    prob_loss = (lbbox_prob_loss + sbbox_prob_loss + mbbox_prob_loss) * 1\n\n    return ciou_loss+conf_loss+prob_loss\n\n\ndef loss_layer(conv, pred, label, bboxes, stride, num_class, iou_loss_thresh):\n    conv_shape = tf.shape(conv)\n    batch_size = conv_shape[0]\n    output_size = conv_shape[1]\n    input_size = stride * output_size\n    conv = tf.reshape(conv, (batch_size, output_size, output_size,\n                             3, 5 + num_class))\n    conv_raw_prob = conv[:, :, :, :, 5:]\n    conv_raw_conf = conv[:, :, :, :, 4:5]\n\n    pred_xywh = pred[:, :, :, :, 0:4]\n    pred_conf = pred[:, :, :, :, 4:5]\n\n    label_xywh = label[:, :, :, :, 0:4]\n    respond_bbox = label[:, :, :, :, 4:5]\n    label_prob = label[:, :, :, :, 5:]\n\n    # Coordinate loss\n    ciou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)  # (8, 13, 13, 3, 1)\n    # ciou = tf.expand_dims(bbox_ciou(pred_xywh, label_xywh), axis=-1)  # (8, 13, 13, 3, 1)\n    input_size = tf.cast(input_size, tf.float32)\n\n    # loss weight of the gt bbox: 2-(gt area/img area)\n    bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\n    ciou_loss = respond_bbox * bbox_loss_scale * (1 - ciou)  # iou loss for respond bbox\n\n    # Classification loss for respond bbox\n    prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\n\n    expand_pred_xywh = pred_xywh[:, :, :, :, np.newaxis, :]  # (?, grid_h, grid_w, 3, 1, 4)\n    expand_bboxes = bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :]  # (?, 1, 1, 1, 70, 4)\n    iou = bbox_iou(expand_pred_xywh, expand_bboxes)  # IoU between all pred bbox and all gt (?, grid_h, grid_w, 3, 70)\n    max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)  # max iou: (?, grid_h, grid_w, 3, 1)\n\n    # ignore the bbox which is not respond bbox and max iou < threshold\n    respond_bgd = (1.0 - respond_bbox) * tf.cast(max_iou < iou_loss_thresh, tf.float32)\n\n    # Confidence loss\n    conf_focal = tf.pow(respond_bbox - pred_conf, 2)\n\n    conf_loss = conf_focal * (\n            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n            +\n            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\n    )\n\n    ciou_loss = tf.reduce_mean(tf.reduce_sum(ciou_loss, axis=[1, 2, 3, 4]))\n    conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1, 2, 3, 4]))\n    prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1, 2, 3, 4]))\n\n    return ciou_loss, conf_loss, prob_loss\n\n\ndef decode(conv_output, anchors, stride, num_class):\n    conv_shape = tf.shape(conv_output)\n    batch_size = conv_shape[0]\n    output_size = conv_shape[1]\n    anchor_per_scale = len(anchors)\n    conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, anchor_per_scale, 5 + num_class))\n    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]\n    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]\n    conv_raw_conf = conv_output[:, :, :, :, 4:5]\n    conv_raw_prob = conv_output[:, :, :, :, 5:]\n    y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size])\n    x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1])\n    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\n    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, anchor_per_scale, 1])\n    xy_grid = tf.cast(xy_grid, tf.float32)\n    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * stride\n    pred_wh = (tf.exp(conv_raw_dwdh) * anchors)\n    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\n    pred_conf = tf.sigmoid(conv_raw_conf)\n    pred_prob = tf.sigmoid(conv_raw_prob)\n    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\n\ndef conv(x, filters, kernel_size, downsampling=False, activation='leaky', batch_norm=True):\n    def mish(x):\n        return x * tf.math.tanh(tf.math.softplus(x))\n\n    if downsampling:\n        x = layers.ZeroPadding2D(padding=((1, 0), (1, 0)))(x)  # top & left padding\n        padding = 'valid'\n        strides = 2\n    else:\n        padding = 'same'\n        strides = 1\n    x = layers.Conv2D(filters,\n                      kernel_size,\n                      strides=strides,\n                      padding=padding,\n                      use_bias=not batch_norm,\n                      # kernel_regularizer=regularizers.l2(0.0005),\n                      kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n                      # bias_initializer=initializers.Zeros()\n                      )(x)\n    if batch_norm:\n        x = layers.BatchNormalization()(x)\n    if activation == 'mish':\n        x = mish(x)\n    elif activation == 'leaky':\n        x = layers.LeakyReLU(alpha=0.1)(x)\n    return x\n\n\ndef residual_block(x, filters1, filters2, activation='leaky'):\n    \"\"\"\n    :param x: input tensor\n    :param filters1: num of filter for 1x1 conv\n    :param filters2: num of filter for 3x3 conv\n    :param activation: default activation function: leaky relu\n    :return:\n    \"\"\"\n    y = conv(x, filters1, kernel_size=1, activation=activation)\n    y = conv(y, filters2, kernel_size=3, activation=activation)\n    return layers.Add()([x, y])\n\n\ndef csp_block(x, residual_out, repeat, residual_bottleneck=False):\n    \"\"\"\n    Cross Stage Partial Network (CSPNet)\n    transition_bottleneck_dims: 1x1 bottleneck\n    output_dims: 3x3\n    :param x:\n    :param residual_out:\n    :param repeat:\n    :param residual_bottleneck:\n    :return:\n    \"\"\"\n    route = x\n    route = conv(route, residual_out, 1, activation=\"mish\")\n    x = conv(x, residual_out, 1, activation=\"mish\")\n    for i in range(repeat):\n        x = residual_block(x,\n                           residual_out // 2 if residual_bottleneck else residual_out,\n                           residual_out,\n                           activation=\"mish\")\n    x = conv(x, residual_out, 1, activation=\"mish\")\n\n    x = layers.Concatenate()([x, route])\n    return x\n\n\ndef darknet53(x):\n    x = conv(x, 32, 3)\n    x = conv(x, 64, 3, downsampling=True)\n\n    for i in range(1):\n        x = residual_block(x, 32, 64)\n    x = conv(x, 128, 3, downsampling=True)\n\n    for i in range(2):\n        x = residual_block(x, 64, 128)\n    x = conv(x, 256, 3, downsampling=True)\n\n    for i in range(8):\n        x = residual_block(x, 128, 256)\n    route_1 = x\n    x = conv(x, 512, 3, downsampling=True)\n\n    for i in range(8):\n        x = residual_block(x, 256, 512)\n    route_2 = x\n    x = conv(x, 1024, 3, downsampling=True)\n\n    for i in range(4):\n        x = residual_block(x, 512, 1024)\n\n    return route_1, route_2, x\n\n\ndef cspdarknet53(input):\n    x = conv(input, 32, 3)\n    x = conv(x, 64, 3, downsampling=True)\n\n    x = csp_block(x, residual_out=64, repeat=1, residual_bottleneck=True)\n    x = conv(x, 64, 1, activation='mish')\n    x = conv(x, 128, 3, activation='mish', downsampling=True)\n\n    x = csp_block(x, residual_out=64, repeat=2)\n    x = conv(x, 128, 1, activation='mish')\n    x = conv(x, 256, 3, activation='mish', downsampling=True)\n\n    x = csp_block(x, residual_out=128, repeat=8)\n    x = conv(x, 256, 1, activation='mish')\n    route0 = x\n    x = conv(x, 512, 3, activation='mish', downsampling=True)\n\n    x = csp_block(x, residual_out=256, repeat=8)\n    x = conv(x, 512, 1, activation='mish')\n    route1 = x\n    x = conv(x, 1024, 3, activation='mish', downsampling=True)\n\n    x = csp_block(x, residual_out=512, repeat=4)\n\n    x = conv(x, 1024, 1, activation=\"mish\")\n\n    x = conv(x, 512, 1)\n    x = conv(x, 1024, 3)\n    x = conv(x, 512, 1)\n\n    x = layers.Concatenate()([layers.MaxPooling2D(pool_size=13, strides=1, padding='same')(x),\n                              layers.MaxPooling2D(pool_size=9, strides=1, padding='same')(x),\n                              layers.MaxPooling2D(pool_size=5, strides=1, padding='same')(x),\n                              x\n                              ])\n    x = conv(x, 512, 1)\n    x = conv(x, 1024, 3)\n    route2 = conv(x, 512, 1)\n    return models.Model(input, [route0, route1, route2])\n\n\ndef yolov4_neck(x, num_classes):\n    backbone_model = cspdarknet53(x)\n    route0, route1, route2 = backbone_model.output\n\n    route_input = route2\n    x = conv(route2, 256, 1)\n    x = layers.UpSampling2D()(x)\n    route1 = conv(route1, 256, 1)\n    x = layers.Concatenate()([route1, x])\n\n    x = conv(x, 256, 1)\n    x = conv(x, 512, 3)\n    x = conv(x, 256, 1)\n    x = conv(x, 512, 3)\n    x = conv(x, 256, 1)\n\n    route1 = x\n    x = conv(x, 128, 1)\n    x = layers.UpSampling2D()(x)\n    route0 = conv(route0, 128, 1)\n    x = layers.Concatenate()([route0, x])\n\n    x = conv(x, 128, 1)\n    x = conv(x, 256, 3)\n    x = conv(x, 128, 1)\n    x = conv(x, 256, 3)\n    x = conv(x, 128, 1)\n\n    route0 = x\n    x = conv(x, 256, 3)\n    conv_sbbox = conv(x, 3 * (num_classes + 5), 1, activation=None, batch_norm=False)\n\n    x = conv(route0, 256, 3, downsampling=True)\n    x = layers.Concatenate()([x, route1])\n\n    x = conv(x, 256, 1)\n    x = conv(x, 512, 3)\n    x = conv(x, 256, 1)\n    x = conv(x, 512, 3)\n    x = conv(x, 256, 1)\n\n    route1 = x\n    x = conv(x, 512, 3)\n    conv_mbbox = conv(x, 3 * (num_classes + 5), 1, activation=None, batch_norm=False)\n\n    x = conv(route1, 512, 3, downsampling=True)\n    x = layers.Concatenate()([x, route_input])\n\n    x = conv(x, 512, 1)\n    x = conv(x, 1024, 3)\n    x = conv(x, 512, 1)\n    x = conv(x, 1024, 3)\n    x = conv(x, 512, 1)\n\n    x = conv(x, 1024, 3)\n    conv_lbbox = conv(x, 3 * (num_classes + 5), 1, activation=None, batch_norm=False)\n\n    return [conv_sbbox, conv_mbbox, conv_lbbox]\n\n\ndef yolov4_head(yolo_neck_outputs, classes, anchors, xyscale):\n    bbox0, object_probability0, class_probabilities0, pred_box0 = get_boxes(yolo_neck_outputs[0],\n                                                                            anchors=anchors[0, :, :], classes=classes,\n                                                                            grid_size=52, strides=8,\n                                                                            xyscale=xyscale[0])\n    bbox1, object_probability1, class_probabilities1, pred_box1 = get_boxes(yolo_neck_outputs[1],\n                                                                            anchors=anchors[1, :, :], classes=classes,\n                                                                            grid_size=26, strides=16,\n                                                                            xyscale=xyscale[1])\n    bbox2, object_probability2, class_probabilities2, pred_box2 = get_boxes(yolo_neck_outputs[2],\n                                                                            anchors=anchors[2, :, :], classes=classes,\n                                                                            grid_size=13, strides=32,\n                                                                            xyscale=xyscale[2])\n    x = [bbox0, object_probability0, class_probabilities0, pred_box0,\n         bbox1, object_probability1, class_probabilities1, pred_box1,\n         bbox2, object_probability2, class_probabilities2, pred_box2]\n\n    return x\n\n\ndef get_boxes(pred, anchors, classes, grid_size, strides, xyscale):\n    \"\"\"\n    :param pred:\n    :param anchors:\n    :param classes:\n    :param grid_size:\n    :param strides:\n    :param xyscale:\n    :return:\n    \"\"\"\n    pred = tf.reshape(pred,\n                      (tf.shape(pred)[0],\n                       grid_size,\n                       grid_size,\n                       3,\n                       5 + classes))  # (batch_size, grid_size, grid_size, 3, 5+classes)\n    box_xy, box_wh, obj_prob, class_prob = tf.split(\n        pred, (2, 2, 1, classes), axis=-1\n    )  # (?, 52, 52, 3, 2) (?, 52, 52, 3, 2) (?, 52, 52, 3, 1) (?, 52, 52, 3, 80)\n\n    box_xy = tf.sigmoid(box_xy)  # (?, 52, 52, 3, 2)\n    obj_prob = tf.sigmoid(obj_prob)  # (?, 52, 52, 3, 1)\n    class_prob = tf.sigmoid(class_prob)  # (?, 52, 52, 3, 80)\n    pred_box_xywh = tf.concat((box_xy, box_wh), axis=-1)  # (?, 52, 52, 3, 4)\n\n    grid = tf.meshgrid(tf.range(grid_size), tf.range(grid_size))  # (52, 52) (52, 52)\n    grid = tf.expand_dims(tf.stack(grid, axis=-1), axis=2)  # (52, 52, 1, 2)\n    grid = tf.cast(grid, dtype=tf.float32)\n\n    box_xy = ((box_xy * xyscale) - 0.5 * (xyscale - 1) + grid) * strides  # (?, 52, 52, 1, 4)\n\n    box_wh = tf.exp(box_wh) * anchors  # (?, 52, 52, 3, 2)\n    box_x1y1 = box_xy - box_wh / 2  # (?, 52, 52, 3, 2)\n    box_x2y2 = box_xy + box_wh / 2  # (?, 52, 52, 3, 2)\n    pred_box_x1y1x2y2 = tf.concat([box_x1y1, box_x2y2], axis=-1)  # (?, 52, 52, 3, 4)\n    return pred_box_x1y1x2y2, obj_prob, class_prob, pred_box_xywh\n    # pred_box_x1y1x2y2: absolute xy value\n\n\ndef nms(model_ouputs, input_shape, num_class, iou_threshold=0.413, score_threshold=0.3):\n    \"\"\"\n    Apply Non-Maximum suppression\n    ref: https://www.tensorflow.org/api_docs/python/tf/image/combined_non_max_suppression\n    :param model_ouputs: yolo model model_ouputs\n    :param input_shape: size of input image\n    :return: nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections\n    \"\"\"\n    bs = tf.shape(model_ouputs[0])[0]\n    boxes = tf.zeros((bs, 0, 4))\n    confidence = tf.zeros((bs, 0, 1))\n    class_probabilities = tf.zeros((bs, 0, num_class))\n\n    for output_idx in range(0, len(model_ouputs), 4):\n        output_xy = model_ouputs[output_idx]\n        output_conf = model_ouputs[output_idx + 1]\n        output_classes = model_ouputs[output_idx + 2]\n        boxes = tf.concat([boxes, tf.reshape(output_xy, (bs, -1, 4))], axis=1)\n        confidence = tf.concat([confidence, tf.reshape(output_conf, (bs, -1, 1))], axis=1)\n        class_probabilities = tf.concat([class_probabilities, tf.reshape(output_classes, (bs, -1, num_class))], axis=1)\n\n    scores = confidence * class_probabilities\n    boxes = tf.expand_dims(boxes, axis=-2)\n    boxes = boxes / input_shape[0]  # box normalization: relative img size\n#     print(f'nms iou: {iou_threshold} score: {score_threshold}')\n    (nmsed_boxes,      # [bs, max_detections, 4]\n     nmsed_scores,     # [bs, max_detections]\n     nmsed_classes,    # [bs, max_detections]\n     valid_detections  # [batch_size]\n     ) = tf.image.combined_non_max_suppression(\n        boxes=boxes,  # y1x1, y2x2 [0~1]\n        scores=scores,\n        max_output_size_per_class=100,\n        max_total_size=100,  # max_boxes: Maximum nmsed_boxes in a single img.\n        iou_threshold=iou_threshold,  # iou_threshold: Minimum overlap that counts as a valid detection.\n        score_threshold=score_threshold,  # # Minimum confidence that counts as a valid detection.\n    )\n    return nmsed_boxes, nmsed_scores, nmsed_classes, valid_detections\n\nyolo_config = {\n    # Basic\n    'img_size': (416, 416, 3),\n    'anchors': [12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401],\n    'strides': [8, 16, 32],\n    'xyscale': [1.2, 1.1, 1.05],\n\n    # Training\n    'iou_loss_thresh': 0.5,\n    'batch_size': 8,\n    'num_gpu': 1,  # 2,\n\n    # Inference\n    'max_boxes': 100,\n    'iou_threshold': 0.413,\n    'score_threshold': 0.3,\n}\n\nclass Yolov4(object):\n    def __init__(self,\n                 weight_path=None,\n                 class_name_path='coco_classes.txt',\n                 config=yolo_config,\n                 ):\n        assert config['img_size'][0] == config['img_size'][1], 'not support yet'\n        assert config['img_size'][0] % config['strides'][-1] == 0, 'must be a multiple of last stride'\n        self.class_names = [line.strip() for line in open(class_name_path).readlines()]\n        self.img_size = yolo_config['img_size']\n        self.num_classes = len(self.class_names)\n        self.weight_path = weight_path\n        self.anchors = np.array(yolo_config['anchors']).reshape((3, 3, 2))\n        self.xyscale = yolo_config['xyscale']\n        self.strides = yolo_config['strides']\n        self.output_sizes = [self.img_size[0] // s for s in self.strides]\n        self.class_color = {name: list(np.random.random(size=3)*255) for name in self.class_names}\n        # Training\n        self.max_boxes = yolo_config['max_boxes']\n        self.iou_loss_thresh = yolo_config['iou_loss_thresh']\n        self.config = yolo_config\n        assert self.num_classes > 0, 'no classes detected!'\n\n        tf.keras.backend.clear_session()\n        if yolo_config['num_gpu'] > 1:\n            mirrored_strategy = tf.distribute.MirroredStrategy()\n            with mirrored_strategy.scope():\n                self.build_model(load_pretrained=True if self.weight_path else False)\n        else:\n            self.build_model(load_pretrained=True if self.weight_path else False)\n\n    def build_model(self, load_pretrained=True):\n        # core yolo model\n        input_layer = layers.Input(self.img_size)\n        yolov4_output = yolov4_neck(input_layer, self.num_classes)\n        self.yolo_model = models.Model(input_layer, yolov4_output)\n\n        # Build training model\n        y_true = [\n            layers.Input(name='input_2', shape=(52, 52, 3, (self.num_classes + 5))),  # label small boxes\n            layers.Input(name='input_3', shape=(26, 26, 3, (self.num_classes + 5))),  # label medium boxes\n            layers.Input(name='input_4', shape=(13, 13, 3, (self.num_classes + 5))),  # label large boxes\n            layers.Input(name='input_5', shape=(self.max_boxes, 4)),  # true bboxes\n        ]\n        loss_list = tf.keras.layers.Lambda(yolo_loss, name='yolo_loss',\n                                           arguments={'num_classes': self.num_classes,\n                                                      'iou_loss_thresh': self.iou_loss_thresh,\n                                                      'anchors': self.anchors})([*self.yolo_model.output, *y_true])\n        self.training_model = models.Model([self.yolo_model.input, *y_true], loss_list)\n\n        # Build inference model\n        yolov4_output = yolov4_head(yolov4_output, self.num_classes, self.anchors, self.xyscale)\n        # output: [boxes, scores, classes, valid_detections]\n        self.inference_model = models.Model(input_layer,\n                                            nms(yolov4_output, self.img_size, self.num_classes,\n                                                iou_threshold=self.config['iou_threshold'],\n                                                score_threshold=self.config['score_threshold']))\n\n        if load_pretrained and self.weight_path and self.weight_path.endswith('.weights'):\n            if self.weight_path.endswith('.weights'):\n                load_weights(self.yolo_model, self.weight_path)\n                print(f'load from {self.weight_path}')\n            elif self.weight_path.endswith('.h5'):\n                self.training_model.load_weights(self.weight_path)\n                print(f'load from {self.weight_path}')\n\n        self.training_model.compile(optimizer=optimizers.Adam(lr=1e-3),\n                                    loss={'yolo_loss': lambda y_true, y_pred: y_pred})\n\n    def load_model(self, path):\n        self.yolo_model = models.load_model(path, compile=False)\n        yolov4_output = yolov4_head(self.yolo_model.output, self.num_classes, self.anchors, self.xyscale)\n        self.inference_model = models.Model(self.yolo_model.input,\n                                            nms(yolov4_output, self.img_size, self.num_classes))  # [boxes, scores, classes, valid_detections]\n\n    def save_model(self, path):\n        self.yolo_model.save(path)\n\n    def preprocess_img(self, img):\n        img = cv2.resize(img, self.img_size[:2])\n        img = img / 255.\n        return img\n\n    def fit(self, train_data_gen, epochs, val_data_gen=None, initial_epoch=0, callbacks=None):\n        self.training_model.fit(train_data_gen,\n                                steps_per_epoch=len(train_data_gen),\n                                validation_data=val_data_gen,\n                                validation_steps=len(val_data_gen),\n                                epochs=epochs,\n                                callbacks=callbacks,\n                                initial_epoch=initial_epoch)\n    # raw_img: RGB\n    def predict_img(self, raw_img, random_color=True, plot_img=True, figsize=(10, 10), show_text=True, return_output=False):\n#         print('img shape: ', raw_img.shape)\n        img = self.preprocess_img(raw_img)\n        imgs = np.expand_dims(img, axis=0)\n        pred_output = self.inference_model.predict(imgs)\n        detections = get_detection_data(img=raw_img,\n                                        model_outputs=pred_output,\n                                        class_names=self.class_names)\n\n        output_img = draw_bbox(raw_img, detections, cmap=self.class_color, random_color=random_color, figsize=figsize,\n                  show_text=show_text, show_img=plot_img)\n        if return_output:\n            return output_img, detections\n        else:\n            return detections\n\n    def predict(self, img_path, random_color=True, plot_img=True, figsize=(10, 10), show_text=True):\n        raw_img = cv2.imread(img_path)[:, :, ::-1]\n        return self.predict_img(raw_img, random_color, plot_img, figsize, show_text)\n\n    def export_gt(self, annotation_path, gt_folder_path):\n        with open(annotation_path) as file:\n            for line in file:\n                line = line.split(' ')\n                filename = line[0].split(os.sep)[-1].split('.')[0]\n                objs = line[1:]\n                # export txt file\n                with open(os.path.join(gt_folder_path, filename + '.txt'), 'w') as output_file:\n                    for obj in objs:\n                        x_min, y_min, x_max, y_max, class_id = [float(o) for o in obj.strip().split(',')]\n                        output_file.write(f'{self.class_names[int(class_id)]} {x_min} {y_min} {x_max} {y_max}\\n')\n\n    def export_prediction(self, annotation_path, pred_folder_path, img_folder_path, bs=2):\n        with open(annotation_path) as file:\n            img_paths = [os.path.join(img_folder_path, line.split(' ')[0].split(os.sep)[-1]) for line in file]\n            # print(img_paths[:20])\n            for batch_idx in tqdm(range(0, len(img_paths), bs)):\n                # print(len(img_paths), batch_idx, batch_idx*bs, (batch_idx+1)*bs)\n                paths = img_paths[batch_idx:batch_idx+bs]\n                # print(paths)\n                # read and process img\n                imgs = np.zeros((len(paths), *self.img_size))\n                raw_img_shapes = []\n                for j, path in enumerate(paths):\n                    img = cv2.imread(path)\n                    raw_img_shapes.append(img.shape)\n                    img = self.preprocess_img(img)\n                    imgs[j] = img\n\n                # process batch output\n                b_boxes, b_scores, b_classes, b_valid_detections = self.inference_model.predict(imgs)\n                for k in range(len(paths)):\n                    num_boxes = b_valid_detections[k]\n                    raw_img_shape = raw_img_shapes[k]\n                    boxes = b_boxes[k, :num_boxes]\n                    classes = b_classes[k, :num_boxes]\n                    scores = b_scores[k, :num_boxes]\n                    # print(raw_img_shape)\n                    boxes[:, [0, 2]] = (boxes[:, [0, 2]] * raw_img_shape[1])  # w\n                    boxes[:, [1, 3]] = (boxes[:, [1, 3]] * raw_img_shape[0])  # h\n                    cls_names = [self.class_names[int(c)] for c in classes]\n                    # print(raw_img_shape, boxes.astype(int), cls_names, scores)\n\n                    img_path = paths[k]\n                    filename = img_path.split(os.sep)[-1].split('.')[0]\n                    # print(filename)\n                    output_path = os.path.join(pred_folder_path, filename+'.txt')\n                    with open(output_path, 'w') as pred_file:\n                        for box_idx in range(num_boxes):\n                            b = boxes[box_idx]\n                            pred_file.write(f'{cls_names[box_idx]} {scores[box_idx]} {b[0]} {b[1]} {b[2]} {b[3]}\\n')\n\n\n    def eval_map(self, gt_folder_path, pred_folder_path, temp_json_folder_path, output_files_path):\n        \"\"\"Process Gt\"\"\"\n        ground_truth_files_list = glob(gt_folder_path + '/*.txt')\n        assert len(ground_truth_files_list) > 0, 'no ground truth file'\n        ground_truth_files_list.sort()\n        # dictionary with counter per class\n        gt_counter_per_class = {}\n        counter_images_per_class = {}\n\n        gt_files = []\n        for txt_file in ground_truth_files_list:\n            file_id = txt_file.split(\".txt\", 1)[0]\n            file_id = os.path.basename(os.path.normpath(file_id))\n            # check if there is a correspondent detection-results file\n            temp_path = os.path.join(pred_folder_path, (file_id + \".txt\"))\n            assert os.path.exists(temp_path), \"Error. File not found: {}\\n\".format(temp_path)\n            lines_list = read_txt_to_list(txt_file)\n            # create ground-truth dictionary\n            bounding_boxes = []\n            is_difficult = False\n            already_seen_classes = []\n            for line in lines_list:\n                class_name, left, top, right, bottom = line.split()\n                # check if class is in the ignore list, if yes skip\n                bbox = left + \" \" + top + \" \" + right + \" \" + bottom\n                bounding_boxes.append({\"class_name\": class_name, \"bbox\": bbox, \"used\": False})\n                # count that object\n                if class_name in gt_counter_per_class:\n                    gt_counter_per_class[class_name] += 1\n                else:\n                    # if class didn't exist yet\n                    gt_counter_per_class[class_name] = 1\n\n                if class_name not in already_seen_classes:\n                    if class_name in counter_images_per_class:\n                        counter_images_per_class[class_name] += 1\n                    else:\n                        # if class didn't exist yet\n                        counter_images_per_class[class_name] = 1\n                    already_seen_classes.append(class_name)\n\n            # dump bounding_boxes into a \".json\" file\n            new_temp_file = os.path.join(temp_json_folder_path, file_id+\"_ground_truth.json\") #TEMP_FILES_PATH + \"/\" + file_id + \"_ground_truth.json\"\n            gt_files.append(new_temp_file)\n            with open(new_temp_file, 'w') as outfile:\n                json.dump(bounding_boxes, outfile)\n\n        gt_classes = list(gt_counter_per_class.keys())\n        # let's sort the classes alphabetically\n        gt_classes = sorted(gt_classes)\n        n_classes = len(gt_classes)\n#         print(gt_classes, gt_counter_per_class)\n\n        \"\"\"Process prediction\"\"\"\n\n        dr_files_list = sorted(glob(os.path.join(pred_folder_path, '*.txt')))\n\n        for class_index, class_name in enumerate(gt_classes):\n            bounding_boxes = []\n            for txt_file in dr_files_list:\n                # the first time it checks if all the corresponding ground-truth files exist\n                file_id = txt_file.split(\".txt\", 1)[0]\n                file_id = os.path.basename(os.path.normpath(file_id))\n                temp_path = os.path.join(gt_folder_path, (file_id + \".txt\"))\n                if class_index == 0:\n                    if not os.path.exists(temp_path):\n                        error_msg = f\"Error. File not found: {temp_path}\\n\"\n                        print(error_msg)\n                lines = read_txt_to_list(txt_file)\n                for line in lines:\n                    try:\n                        tmp_class_name, confidence, left, top, right, bottom = line.split()\n                    except ValueError:\n                        error_msg = f\"\"\"Error: File {txt_file} in the wrong format.\\n \n                                        Expected: <class_name> <confidence> <left> <top> <right> <bottom>\\n \n                                        Received: {line} \\n\"\"\"\n                        print(error_msg)\n                    if tmp_class_name == class_name:\n                        # print(\"match\")\n                        bbox = left + \" \" + top + \" \" + right + \" \" + bottom\n                        bounding_boxes.append({\"confidence\": confidence, \"file_id\": file_id, \"bbox\": bbox})\n            # sort detection-results by decreasing confidence\n            bounding_boxes.sort(key=lambda x: float(x['confidence']), reverse=True)\n            with open(temp_json_folder_path + \"/\" + class_name + \"_dr.json\", 'w') as outfile:\n                json.dump(bounding_boxes, outfile)\n\n        \"\"\"\n         Calculate the AP for each class\n        \"\"\"\n        sum_AP = 0.0\n        ap_dictionary = {}\n        # open file to store the output\n        with open(output_files_path + \"/output.txt\", 'w') as output_file:\n            output_file.write(\"# AP and precision/recall per class\\n\")\n            count_true_positives = {}\n            for class_index, class_name in enumerate(gt_classes):\n                count_true_positives[class_name] = 0\n                \"\"\"\n                 Load detection-results of that class\n                \"\"\"\n                dr_file = temp_json_folder_path + \"/\" + class_name + \"_dr.json\"\n                dr_data = json.load(open(dr_file))\n\n                \"\"\"\n                 Assign detection-results to ground-truth objects\n                \"\"\"\n                nd = len(dr_data)\n                tp = [0] * nd  # creates an array of zeros of size nd\n                fp = [0] * nd\n                for idx, detection in enumerate(dr_data):\n                    file_id = detection[\"file_id\"]\n                    gt_file = temp_json_folder_path + \"/\" + file_id + \"_ground_truth.json\"\n                    ground_truth_data = json.load(open(gt_file))\n                    ovmax = -1\n                    gt_match = -1\n                    # load detected object bounding-box\n                    bb = [float(x) for x in detection[\"bbox\"].split()]\n                    for obj in ground_truth_data:\n                        # look for a class_name match\n                        if obj[\"class_name\"] == class_name:\n                            bbgt = [float(x) for x in obj[\"bbox\"].split()]\n                            bi = [max(bb[0], bbgt[0]), max(bb[1], bbgt[1]), min(bb[2], bbgt[2]), min(bb[3], bbgt[3])]\n                            iw = bi[2] - bi[0] + 1\n                            ih = bi[3] - bi[1] + 1\n                            if iw > 0 and ih > 0:\n                                # compute overlap (IoU) = area of intersection / area of union\n                                ua = (bb[2] - bb[0] + 1) * (bb[3] - bb[1] + 1) + \\\n                                     (bbgt[2] - bbgt[0]+ 1) * (bbgt[3] - bbgt[1] + 1) - iw * ih\n                                ov = iw * ih / ua\n                                if ov > ovmax:\n                                    ovmax = ov\n                                    gt_match = obj\n\n                    min_overlap = 0.5\n                    if ovmax >= min_overlap:\n                        # if \"difficult\" not in gt_match:\n                        if not bool(gt_match[\"used\"]):\n                            # true positive\n                            tp[idx] = 1\n                            gt_match[\"used\"] = True\n                            count_true_positives[class_name] += 1\n                            # update the \".json\" file\n                            with open(gt_file, 'w') as f:\n                                f.write(json.dumps(ground_truth_data))\n                        else:\n                            # false positive (multiple detection)\n                            fp[idx] = 1\n                    else:\n                        fp[idx] = 1\n\n\n                # compute precision/recall\n                cumsum = 0\n                for idx, val in enumerate(fp):\n                    fp[idx] += cumsum\n                    cumsum += val\n#                 print('fp ', cumsum)\n                cumsum = 0\n                for idx, val in enumerate(tp):\n                    tp[idx] += cumsum\n                    cumsum += val\n#                 print('tp ', cumsum)\n                rec = tp[:]\n                for idx, val in enumerate(tp):\n                    rec[idx] = float(tp[idx]) / gt_counter_per_class[class_name]\n#                 print('recall ', cumsum)\n                prec = tp[:]\n                for idx, val in enumerate(tp):\n                    prec[idx] = float(tp[idx]) / (fp[idx] + tp[idx])\n#                 print('prec ', cumsum)\n\n                ap, mrec, mprec = voc_ap(rec[:], prec[:])\n                sum_AP += ap\n                text = \"{0:.2f}%\".format(\n                    ap * 100) + \" = \" + class_name + \" AP \"  # class_name + \" AP = {0:.2f}%\".format(ap*100)\n\n#                 print(text)\n                ap_dictionary[class_name] = ap\n\n                n_images = counter_images_per_class[class_name]\n                # lamr, mr, fppi = log_average_miss_rate(np.array(prec), np.array(rec), n_images)\n                # lamr_dictionary[class_name] = lamr\n\n                \"\"\"\n                 Draw plot\n                \"\"\"\n                if True:\n                    plt.plot(rec, prec, '-o')\n                    # add a new penultimate point to the list (mrec[-2], 0.0)\n                    # since the last line segment (and respective area) do not affect the AP value\n                    area_under_curve_x = mrec[:-1] + [mrec[-2]] + [mrec[-1]]\n                    area_under_curve_y = mprec[:-1] + [0.0] + [mprec[-1]]\n                    plt.fill_between(area_under_curve_x, 0, area_under_curve_y, alpha=0.2, edgecolor='r')\n                    # set window title\n                    fig = plt.gcf()  # gcf - get current figure\n                    fig.canvas.set_window_title('AP ' + class_name)\n                    # set plot title\n                    plt.title('class: ' + text)\n                    # plt.suptitle('This is a somewhat long figure title', fontsize=16)\n                    # set axis titles\n                    plt.xlabel('Recall')\n                    plt.ylabel('Precision')\n                    # optional - set axes\n                    axes = plt.gca()  # gca - get current axes\n                    axes.set_xlim([0.0, 1.0])\n                    axes.set_ylim([0.0, 1.05])  # .05 to give some extra space\n                    # Alternative option -> wait for button to be pressed\n                    # while not plt.waitforbuttonpress(): pass # wait for key display\n                    # Alternative option -> normal display\n                    plt.show()\n                    # save the plot\n                    # fig.savefig(output_files_path + \"/classes/\" + class_name + \".png\")\n                    # plt.cla()  # clear axes for next plot\n\n            # if show_animation:\n            #     cv2.destroyAllWindows()\n\n            output_file.write(\"\\n# mAP of all classes\\n\")\n            mAP = sum_AP / n_classes\n            text = \"mAP = {0:.2f}%\".format(mAP * 100)\n            output_file.write(text + \"\\n\")\n#             print(text)\n\n        \"\"\"\n         Count total of detection-results\n        \"\"\"\n        # iterate through all the files\n        det_counter_per_class = {}\n        for txt_file in dr_files_list:\n            # get lines to list\n            lines_list = read_txt_to_list(txt_file)\n            for line in lines_list:\n                class_name = line.split()[0]\n                # check if class is in the ignore list, if yes skip\n                # if class_name in args.ignore:\n                #     continue\n                # count that object\n                if class_name in det_counter_per_class:\n                    det_counter_per_class[class_name] += 1\n                else:\n                    # if class didn't exist yet\n                    det_counter_per_class[class_name] = 1\n        # print(det_counter_per_class)\n        dr_classes = list(det_counter_per_class.keys())\n\n        \"\"\"\n         Plot the total number of occurences of each class in the ground-truth\n        \"\"\"\n        if True:\n            window_title = \"ground-truth-info\"\n            plot_title = \"ground-truth\\n\"\n            plot_title += \"(\" + str(len(ground_truth_files_list)) + \" files and \" + str(n_classes) + \" classes)\"\n            x_label = \"Number of objects per class\"\n            output_path = output_files_path + \"/ground-truth-info.png\"\n            to_show = False\n            plot_color = 'forestgreen'\n            draw_plot_func(\n                gt_counter_per_class,\n                n_classes,\n                window_title,\n                plot_title,\n                x_label,\n                output_path,\n                to_show,\n                plot_color,\n                '',\n            )\n\n        \"\"\"\n         Finish counting true positives\n        \"\"\"\n        for class_name in dr_classes:\n            # if class exists in detection-result but not in ground-truth then there are no true positives in that class\n            if class_name not in gt_classes:\n                count_true_positives[class_name] = 0\n        # print(count_true_positives)\n\n        \"\"\"\n         Plot the total number of occurences of each class in the \"detection-results\" folder\n        \"\"\"\n        if True:\n            window_title = \"detection-results-info\"\n            # Plot title\n            plot_title = \"detection-results\\n\"\n            plot_title += \"(\" + str(len(dr_files_list)) + \" files and \"\n            count_non_zero_values_in_dictionary = sum(int(x) > 0 for x in list(det_counter_per_class.values()))\n            plot_title += str(count_non_zero_values_in_dictionary) + \" detected classes)\"\n            # end Plot title\n            x_label = \"Number of objects per class\"\n            output_path = output_files_path + \"/detection-results-info.png\"\n            to_show = False\n            plot_color = 'forestgreen'\n            true_p_bar = count_true_positives\n            draw_plot_func(\n                det_counter_per_class,\n                len(det_counter_per_class),\n                window_title,\n                plot_title,\n                x_label,\n                output_path,\n                to_show,\n                plot_color,\n                true_p_bar\n            )\n\n        \"\"\"\n         Draw mAP plot (Show AP's of all classes in decreasing order)\n        \"\"\"\n        if True:\n            window_title = \"mAP\"\n            plot_title = \"mAP = {0:.2f}%\".format(mAP * 100)\n            x_label = \"Average Precision\"\n            output_path = output_files_path + \"/mAP.png\"\n            to_show = True\n            plot_color = 'royalblue'\n            draw_plot_func(\n                ap_dictionary,\n                n_classes,\n                window_title,\n                plot_title,\n                x_label,\n                output_path,\n                to_show,\n                plot_color,\n                \"\"\n            )\n\n    def predict_raw(self, img_path):\n        raw_img = cv2.imread(img_path)\n#         print('img shape: ', raw_img.shape)\n        img = self.preprocess_img(raw_img)\n        imgs = np.expand_dims(img, axis=0)\n        return self.yolo_model.predict(imgs)\n\n    def predict_nonms(self, img_path, iou_threshold=0.413, score_threshold=0.1):\n        raw_img = cv2.imread(img_path)\n#         print('img shape: ', raw_img.shape)\n        img = self.preprocess_img(raw_img)\n        imgs = np.expand_dims(img, axis=0)\n        yolov4_output = self.yolo_model.predict(imgs)\n        output = yolov4_head(yolov4_output, self.num_classes, self.anchors, self.xyscale)\n        pred_output = nms(output, self.img_size, self.num_classes, iou_threshold, score_threshold)\n        pred_output = [p.numpy() for p in pred_output]\n        detections = get_detection_data(img=raw_img,\n                                        model_outputs=pred_output,\n                                        class_names=self.class_names)\n        draw_bbox(raw_img, detections, cmap=self.class_color, random_color=True)\n        return detections","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-18T04:59:29.524846Z","iopub.execute_input":"2022-02-18T04:59:29.525484Z","iopub.status.idle":"2022-02-18T04:59:35.196578Z","shell.execute_reply.started":"2022-02-18T04:59:29.525438Z","shell.execute_reply":"2022-02-18T04:59:35.195577Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# PreTrained Yolo Model","metadata":{}},{"cell_type":"code","source":"path = '../input/open-images-2019-object-detection'\nsample_submission = pd.read_csv(\"../input/open-images-2019-object-detection-sample/sample_submission.csv\")\ntest_dir = os.listdir(f\"{path}/test\")\nprint(len(test_dir))\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:59:35.198130Z","iopub.execute_input":"2022-02-18T04:59:35.198457Z","iopub.status.idle":"2022-02-18T04:59:37.066770Z","shell.execute_reply.started":"2022-02-18T04:59:35.198416Z","shell.execute_reply":"2022-02-18T04:59:37.066165Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = Yolov4(weight_path='./yolov4.weights', \n               class_name_path='./yolo-v4-tf.keras/class_names/coco_classes.txt')\npred = model.predict(f\"{path}/test/{test_dir[0]}\")\npred","metadata":{"papermill":{"duration":0.38981,"end_time":"2022-02-16T08:56:02.231874","exception":true,"start_time":"2022-02-16T08:56:01.842064","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-18T04:59:37.069200Z","iopub.execute_input":"2022-02-18T04:59:37.070048Z","iopub.status.idle":"2022-02-18T04:59:46.759846Z","shell.execute_reply.started":"2022-02-18T04:59:37.070007Z","shell.execute_reply":"2022-02-18T04:59:46.759252Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(sample_submission.loc[0,'PredictionString'],sample_submission.loc[0,'ImageId'])\npred = model.predict(f\"{path}/test/{sample_submission.loc[0,'ImageId']}.jpg\")\npred","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2022-02-18T04:59:46.761166Z","iopub.execute_input":"2022-02-18T04:59:46.761401Z","iopub.status.idle":"2022-02-18T04:59:48.098518Z","shell.execute_reply.started":"2022-02-18T04:59:46.761371Z","shell.execute_reply":"2022-02-18T04:59:48.097681Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(f\"{path}/test/{sample_submission.loc[0,'ImageId']}.jpg\",show_text=False,plot_img=False)\n# pred","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:59:48.099660Z","iopub.execute_input":"2022-02-18T04:59:48.099880Z","iopub.status.idle":"2022-02-18T04:59:49.002025Z","shell.execute_reply.started":"2022-02-18T04:59:48.099851Z","shell.execute_reply":"2022-02-18T04:59:49.001131Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# img_ids = sample_submission.ImageId.values.tolist()\n# img_shapes = []\n# for img_id in tqdm(img_ids) :\n# #     final_preds.append(format_op_string(img_id))\n#     img = cv2.imread(f\"{path}/test/{img_id}.jpg\")\n#     img_shapes.append(img.shape)\n# #     break\n# len(img_shapes)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:59:49.004267Z","iopub.execute_input":"2022-02-18T04:59:49.004504Z","iopub.status.idle":"2022-02-18T04:59:49.008520Z","shell.execute_reply.started":"2022-02-18T04:59:49.004473Z","shell.execute_reply":"2022-02-18T04:59:49.007694Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def format_op_string(img_id,x,y) :\n    df_pred = model.predict(f\"{path}/test/{img_id}.jpg\",show_text=False,plot_img=False)\n    ans = \"\"\n    for i in df_pred.iterrows():\n        x1,x2,y1,y2 = i[1]['x1'],i[1]['x2'],i[1]['y1'],i[1]['y2']\n        s = i[1]['score']\n        x1 /= x\n        x2 /= x\n        y1 /= y\n        y2 /= y\n        try :\n            ans += f\"{rev[i[1]['class_name']]} {s} {x1} {y1} {x2} {y2} \"\n        except :\n            pass\n#         print(i[1])\n    return ans\n\nimg_ids = sample_submission.ImageId.values.tolist()\nfinal_preds = []\nfor i,img_id in tqdm(enumerate(img_ids)) :\n    final_preds.append(format_op_string(img_id,sample_submission.loc[i,'h'],sample_submission.loc[i,'w']))\n#     print(sample_submission.loc[i,'w'],)\n    break\nlen(final_preds)\n# final_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-18T04:59:49.009803Z","iopub.execute_input":"2022-02-18T04:59:49.010522Z","iopub.status.idle":"2022-02-18T04:59:49.905882Z","shell.execute_reply.started":"2022-02-18T04:59:49.010459Z","shell.execute_reply":"2022-02-18T04:59:49.905299Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}